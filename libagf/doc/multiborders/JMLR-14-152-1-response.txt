Review Decision of JMLR-14-152

I regret to inform you that your submission "Multi-borders
classification" has been rejected from publication at JMLR Track for
Machine Learning Open Source Software (MLOSS).

The JMLR MLOSS track publishes submissions about significant
contributions to machine learning open source software. Our most
important review criteria include novelty, breadth and significance of
the software contribution, together with the quality of the four page
description and the package documentation.

The submission is about libAGF library but focuses on a very specific
aspect of a novel multi-class classification scheme. As such the
submission does not really seem like a pure software paper but rather
a mix between software and theory papers. The theory side of the paper
appears fairly weak, as it fails to cite practically any relevant
prior work. While I am not an expert in the area, at least Allwein et
al. (JMLR 2000) seems very relevant to me. The same is true for the
software side which includes no comparisons to existing
implementations.

-> it was pointed out that most software has support for only one or two
multi-class methods and that e.g. LIBSVM only supports 1-vs-1

The multi-class classification feature appears quite
limited in breadth as based on the description it seems to be limited
to the AGF classifiers.

-> Support for external binary classifiers; support for LIBSVM classifiers
now implemented

Looking at the review criteria (http://jmlr.org/mloss/mloss-info.html)
in more detail, libAGF seems lacking in most areas:
1. Four page description is does not really describe the software.
2. The breadth and significance appear limited.
-> not sure how to address either of above two points...
-> trying to build up a bigger user base by publicizing the software 
   (you asswipe...) (Matthew effect in action...)
3. Seems mostly OK.
4. ? (not evaluated)
5. Seems OK, should specify the license.
-> OK, should fix this; doesn't seem like a big deal...
6. OK
7. Seems poor. No real documentation, just LaTeX sources of papers
   some of which fail to compile.
-> "quickstart" guide has been converted to full-form user manual
8. Seems quite limited.
-> true; should fix this as well...
9. No comparisons reported.
-> not true

Other random issues:
- The name 'libAGF' would usually suggest a library developers can
  link with their own code, not an end user tool like libAGF is.
-> also not true

- The license of the code should be clearly stated in the package.
  The copyright notice "All rights reserved" included in many files
  is clearly not valid for open source.
-> should fix this...

Based on these observations it is clear that the submission does not
meet the standard required for publication in JMLR MLOSS track, and I
must therefore reject it.

If the author feels the proposed multi-borders classification approach
is useful, I would recommend performing the necessary comparisons with
true state-of-the-art and writing a non-software paper about it.

I wish to nevertheless thank you for your interest in JMLR Track for
Machine Learning Open Source Software, and hope you will consider us
again for your possible next submission.


Antti Honkela
Action editor, Journal of Machine Learning Research

Review Criteria

The following guidelines would be used to review submissions. While ideally submissions should satisfy all the criteria below, they are not necessary requirements.
1. The quality of the four page description.
2. The novelty, breadth, and significance of the contribution (including evidence of an active user community).
3. The openness of the project, such as a public source code repository, bug tracker, mailing list/forum, that allows new developers to participate and contribute.
4. The clarity of design.
5. The freedom of the code (lack of dependence on proprietary software).
6. The breadth of platforms it can be used on (should include an open-source operating system).
7. The quality of the user documentation (should enable new users to quickly apply the software to other problems, including a tutorial and several non-trivial examples of how the software can be used).
8. The quality of the developer documentation (should enable easy modification and extension of the software, provide an API reference, provide unit testing routines).
9. The quality of comparison to previous (if any) related implementations, w.r.t. run-time, memory requirements, features, to explain that significant progress has been made.

