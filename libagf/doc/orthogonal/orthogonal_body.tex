
\section{Introduction}

Many methods of statistical classication can only discriminate between two classes. 
Examples include lineear classifiers such as perceptrons and logistic regression \citep{Michie_etal1994}, 
piecewise linear classifiers \citep{Herman_Yeung1992,Mills2011},
as well as support vector machines \citep{kernel_intro}.
There are many ways of generalizing binary classification to 
multi-class.
Three of the most common are one versus one, one versus the rest and 
error-correcting coding matrices \citep{Hsu_Lin2002}.
Here we are interested in the error-correcting coding matrices
\citep{Dietterich_Bakiri1995, Windeatt_Ghaderi2002} and
rather than use a random coding matrix we are interested in one that is
more carefully designed.

In error-correcting coding, there is a coding matrix, $A$, that specifies
how the set of multiple classes is partitioned.
Typically, the class of the test point is determined by the distance between
a column in the matrix and a vector of binary {\it decision functions}:
\begin{equation}
	c(\vec x) = \arg \min_j | \vec a^{(j)} - \vec r(\vec x) |
\end{equation}
where $\vec a^{(j)}$ is the $j$th column of the coding matrix and $\vec r$
is a vector of decision functions at {\it test point}, $\vec x$.
If we take the upright brackets as a Euclidean distance, and assume that
each partition partitions all of the classes, that is, there are no zeroes
in $A$, then this reduces to a {\it voting} solution:
\begin{equation}
	c = \arg \max A^T \vec r \label{voting}
\end{equation}
Both \citet{Allwein_etal2000} and \citet{Windeatt_Ghaderi2002} show that to
maximize the accuracy of an error-correcting coding matrix, the distance
between each column, $|\vec a^{(i)} - \vec a^{(j)}|_{i \ne j}$ should be as
large as possible.
Using the same assumptions, this reduces to:
\begin{equation}
	\min |\vec a^{(i)} \cdot a^{(j)}|_{i \ne j}
\end{equation}
In other words, the coding matrix, $A$, should be orthogonal.
This approach to the multi-class problem will be described in detail in this note.

\section{Algorithm}

We wish to design a set of $m$ binary classifiers, each of which return a 
decision function:
\begin{equation}
r_j(\vec x) = P_i(-1 | \vec x) - P_i(+1 | \vec x)
\end{equation}
where $P_i(c | \vec x)$ is the conditional probability of the $c$th class of
the $i$th classifier.
Each binary classifier partitions a set of $n$ classes such that for a
given test point, $\vec x$:
\begin{equation}
\sum_j a_{ij} p_j = r_j
\end{equation}
where $A=\lbrace a_{ij} \in \lbrace -1, +1 \rbrace  \rbrace$ is a {\it coding
matrix} and $p_j = p(j | \vec x)$ is the conditional probability of the $j$th
class.
In vector notation:
\begin{equation}
	A \vec p = \vec r \label{inverse}
\end{equation}
The more general case where a class can be excluded, that is each element of 
the coding matrix is, $a_{ij} \in \lbrace -1, 0, +1\rbrace$, 
will not be addressed here.

Note that this assumes that the binary decision functions, $\vec r$,
estimate the conditional probabilities perfectly.
In practice
there are a set of constrainsts that must be enforced
because $\vec p$ is only allowed to take on certain values.
Thus, we wish to solve the following minimization problem:
\begin{eqnarray}
	\vec p & = & \arg \min_{\vec p} | A \vec p - \vec r | \label{minimization}\\
	\sum_j p_j & = & 1 \label{normalization}\\
	p_j & \ge & 0 \label{nonnegative}
\end{eqnarray}

The unconstrained minimization problem is easy to solve if $A$ is orthogonal:
\begin{equation}
	A^T A = m I
\end{equation}
where $I$ is the $n \times n$ identity matrix and $n$ the number of classes.
Note that the voting solution in (\ref{voting}) is now equivalent to
the inverse solution in (\ref{inverse}).
This allows us to determine the class easily, but we also wish to solve for
the probabilities, $\vec p$, so that none of the constraints in 
(\ref{normalization}) or (\ref{nonnegative}) are violated.
Probabilities are useful for gauging the accuracy of a classification result
when its true value is unknown and for recalibrating an image derived from
statistical classification \citep{Fawcett2006,Mills2009,Mills2011}.

The orthogonality property allows us to reduce the minimization problem 
in (\ref{minimization}) to something much simpler:
\begin{equation}
	\vec p = \arg \min_{\vec p} | \vec p - \vec p_0 |
\end{equation}
where $p_0 = A^T \vec r/m$ with the constraints in (\ref{normalization}) and
(\ref{nonnegative}) remaining the same.
Because the system has been rotated and expanded, the non-negativity 
constraints in (\ref{nonnegative}) remain orthogonal, meaning they are 
independent: enforcing one by setting one of the probabilities to zero, 
$p_k=0$ for example, shouldn't otherwise affect the solution.
This still leaves the normalization constraint in (\ref{normalization}):
the problem, now strictly geometrical, is comprised of finding the point nearest $p_0$ on the diagonal hyper-surface that bisects the unit hyper-cube.

Briefly, we can summarize the algorithm as follows:
1. move to the nearest point that satisfies the normalization constraint,
(\ref{normalization}); 2. if one or more of the probabilities is negative,
move to the nearest point that satisfies both 
the normalization constraint
and the non-negativity constraints, (\ref{nonnegative}), for the negative probabilities;
3. repeat step 2.
More formally, let $\vec 1$ be a vector of all $1$'s:
\begin{itemize}
	\item $n_0=n$
	\item while $\exists k \, p_{ik} \le 0 \lor \vec p_i \cdot \vec 1 \ne 1$:
	\begin{itemize}
		\item if $\vec p_i \cdot \vec 1 \ne 1$ then 
		$\vec p_{i+1} = \vec p_i + (\vec p_i \cdot \vec 1 - 1)/n_i$
		\item let $K$ be the set of $k$ such that $p_{i+1,k} < 0$
		\item for each $k$ in $K$:
		\begin{itemize}
			\item $p_{i+1,k}=0$
			\item Remove $k$ from the problem
		\end{itemize}
		\item $n_{i+1}=n_i-|K|$
		\item $i=i+1$
	\end{itemize}
\end{itemize}

Note that the direction vectors form an orthogonal set.
For instance, suppose $n_0=4$ and after enforcing the normalization constraint,
the first probability is less than zero, $p_{1,1} < 0$,
then the direction vectors for the two motions are:
\begin{equation}
	\frac{1}{2}[1, 1, 1, 1] \cdot \frac{1}{2\sqrt{3}} [-3, 1, 1, 1] = 0
\end{equation}

More generally, consider the following sequence of vectors:
\begin{equation}
	v_{ij} = \frac{1}{\sqrt{(n-i)^2-n+i}} \left \lbrace \begin{array}{rl}
			0; & j < i \\
			-n+i+1; & j=i \\
			1; & j > i
		\end{array} \right .
\end{equation}
where $j \in [1, n]$.


\section{Constructing the coding matrix}

An obvious method of designing an orthogonal $A$ is using harmonic series.
Consider the following matrix for eight binary classifiers ($m=8$) and
five classes ($n=5$):
\begin{equation}
	A^T = \left [ \begin{array}{rrrrrrrr}
			-1 & -1 & -1 & -1 & 1 & 1 & 1 & 1 \\
			-1 & -1 & 1 & 1 & 1 & 1 & -1 & -1 \\
			-1 & -1 & 1 & 1 & -1 & -1 & 1 & 1 \\
			-1 & 1 & 1 & -1 & -1 & 1 & 1 & -1 \\
			-1 & 1 & -1 & 1 & -1 & 1 & -1 & 1
	\end{array} \right ]
\end{equation}
This will limit the size of $m$ relative to $n$; more precisely:
$m=2 \log_2 n - 1$.

Finding an $A$ such that $A^T A = m I$ and $a_{ij} \in \lbrace -1, 1, \rbrace$
is quite a difficult combinatorial problem.
Noting which versions of $A$ are degenerate (equivalent) can help us whittle
away at the problem:
\begin{itemize}
	\item $-A$ is equivalent to $A$
	\item re-arranging either the rows or columns of $A$ makes an equivalent $A$: $T_{ij} A$ or $A T_{ij}^T$ are both equivalent to $A$ where $T_{ij}$ is the elementary matrix which exchanges rows $i$ and $j$
\end{itemize}

The most efficient method the author has discovered so far 
%for constructing an orthogonal coding matrix 
is based on a generalization of the cross-product:
each time a new vector is added to the set of mutually orthogonal vectors, 
the sub-space of possible new members is decreased by one.
Thus, if there are $k$ vectors of length $m$ already, then we need only specify $m-k$
of the elements of any new vector.
We can solve for the rest of the elements using sub-matrices:
\begin{equation}
	\sum_{i=1}^{k} a_{ij} v_i = - \sum_{i=k+1}^m a_{ij} v_i
\end{equation}
where $\vec v$ is the candidate new vector of which we have specified the last
$m-k$ elements: there will of course be $2^{m-k}$ possibilities.
If $v_i \in \lbrace -1, +1 \rbrace;~i=[1..k]$ and $\vec v$ contains both positive and
negative elements, then we have found a new column of the matrix.

With this algorithm, you reduce the number of combinations by roughly a square
root versus a pure, brute force algorithm which requires examining $2^{n(m-1)}$
combinations maximum.
%you will have to examine $2^{m(m-1)/2}$ combinations
%maximum.
%With a pure, brute force algorithm, this would be $2^{n(m-1)}$ to examine all
%permutations while for 
For a similarly brute force but combinatorial algorithm 
in which all possible vectors are treated as elements to be chosen for the 
final matrix this would be $2^{m-1}$ choose $n$ or:
\begin{equation}
	\frac{2^{m-1}!}{n!(2^{m-1}-n)!}
\end{equation}
By taking advantage of the negative degeneracy (first point, above) 
we reduce both these by half, hence the use of $m-1$ instead of $m$.

The final point concerns the size of $m$. In order to find on the order
of $n=m$ columns to match the $m$ rows, it appears that $m$ should be 
(at minimum) divisible by $4$: $m \mathrm{mod} 4 = 0$.
Note that we can have more rows than columns but not the other way around.

To compute the results in this note, orthogonal coding matrices were generated
by simple, brute force.
Each row is randomly chosen from a list of all
possible rows represented as binary numbers and then multiplied with 
each of the previous rows.
Obviously, this will only work for relatively few classes.
More work will need to be done to find an efficient method for generating 
orthogonal coding matrices
if the method is to be applied to problems with a large number of classes.

\section{Validating the probabilities}

\begin{figure}
	\includegraphics[width=0.9\textwidth]{val_prob.eps}
	\caption{Validating probabilities for the vehicle dataset
		using equations	(\ref{prob_val1}) and (\ref{prob_val2}).
		Probabilities are generated by solving one-versus-one 
		partitioning.}\label{val_prob}
\end{figure}

Since the main goal of the method 
is to solve for the conditional probabilities, 
we need a good way to validate them. 
Here we use a technique based on the
following theorem, which can be derived from Bayes theorem and the 
definition of probability:
\begin{equation}
	\lim_{n \rightarrow \infty} \sum_{i=0}^{n} \left [
	p(c | \vec x_i) - \delta_{cy_i} \right ] = 0
	\label{theorem}
\end{equation}
where $\delta$ is the Kronecker delta.
To make this useful for real problems we need to do several things.
First, the probability estimates are lumped together and sorted without regard
to the class label, as follows:
\begin{equation}
	p(c_{i+1} | \vec x_{i+1}) \ge p(c_i | \vec x_i)
\end{equation}
Note that in this ordering, there will be $n$ $i$s indexing the same $\vec x$ 
and the same $y$.
Second, we rearrange Equation (\ref{theorem}) as follows:
\begin{equation}
	N^\prime \approx \sum_{i=i_0}^{N^\prime+i_0} \frac{\delta_{c_i y_i}}{p(c_i | \vec x_i)}
	\label{prob_val1}
\end{equation}
In other words, if the probabilities are well estimated, the RHS should
follow, on average, single step intervals at each step in the trace.
Finally, to prevent singularities at the bottom end, 
we divide it into two parts:
one for all the probabilities greater than a certain threshold, above, 
and one for all those less than this threshold, below:
\begin{equation}
	N^\prime  \approx \sum_{i=N^\prime+i_0}^{i_0} \frac{\delta_{c_i y_i} - 1}{1 - p(c_i | \vec x_i)}
	\label{prob_val2}
\end{equation}
where $i_0$ is the index of the threshold which we choose as the
point between ``winning'' and ``losing'' probabilities:
\begin{equation}
	p(c_i | \vec x_i) \ge 1/n \iff i \ge i_0
\end{equation}
Thus, terms in both Equation (\ref{prob_val1}) and (\ref{prob_val2}) are more
often non-zero than zero.

We use the correlation coefficient to gauge the accuracy and the slope to
measure the bias.
This method of validating conditional probabilities was used in
\citet{Mills2009} for a binary classification problem.
A demonstration is shown in Figure \ref{val_prob}
for the vehicle dataset using the one-versus-one method--see next section.

\section{Results}

\begin{table}
\caption{Speed and skill of multi-class classification results.}\label{class_results}
\input{orthogonal_class_results.tex}
\caption{Precision and bias of solved conditional probabilities.}\label{prob_results}
\input{orthogonal_prob_results.tex}
\end{table}

The method was tested on six different datasets:
two for digit recognition--``pendigits'' \citep{Alimoglu1996} and
``usps'' \citep{Hull1994}, the space shuttle control dataset--``shuttle''
\citep{King_etal1995}, a satellite land recognition
dataset--``sat'' \citep{King_etal1995}, 
a similar dataset for image recognition--``segment'' \citep{King_etal1995},
and a dataset for vehicle recognition--``vehicle'' \citep{Siebert1987}.
It was compared with a technique that solves the ``one versus one'' 
partitioning using matrix inversion \citep{Wu_etal2004}.
Both techniques were applied to support vector machines (SVMs) trained using
LIBSVM \citep{Chang_Lin2011}.
Partitions were trained separately then combined by finding the union of
sets of support vectors for each partition.
By indexing into the combined list of support vectors, the algorithms are
optimized in both space and time.

Results are shown in Tables \ref{class_results} and \ref{prob_results}.
Each dataset was randomly separated into 60\% training and 40\%
test data.
Confidence limits represent standard deviations over 20 trials.
``U.C'' stands for uncertainty
coefficient, a skill score based on Shannon's channel capacity
\citep{Shannon,Press_etal1992,Mills2011}.
Probabilities are validated in Table \ref{prob_results} with Pearson correlation and slope
applied to a trace generated by 
Equations (\ref{prob_val1}) and (\ref{prob_val2})--see Figure \ref{val_prob}.

The results suggest that there is very little difference between the two
methods, whether in classification accuracy, accuracy of probabily estimates
or classification time.
On the whole, the one-versus-one method appears to have a slight edge in
classification accuracy as well as both precision and bias in probabilty
estimates.
Differences are small but nonetheless statistically significant.
Probability estimates for both methods appear to be highly accurate.

There is the potential for speed increases for binary classification methods
that don't depend on the number of samples in the training data when using
orthogonal coding matrices.
In this case, time efficiency will depend on the number of classes,
$O(n)$, rather than the number of classes squared, $O(n^2)$.
The obvious example is logistic regression, however there will likely be
a concordant decrease in accuracy for the orthogonal coding matrices since
boundaries between classes will be more complex.

On the other hand, the speed of a SVM is directly proportional to the number 
of support vectors which in turn is proportional to the number of training samples: 
$O(N)$, where $N$ is the number of training samples.
Since each partition with an orthogonal coding matrix includes all of the
training data, time efficiency per partition is proportional to the number
of training samples, whereas in one-versus-one, it's proportional, on average,
to the number of training samples divided by the number of classes: $O(N/n)$.
Hence, both methods will have time efficiency roughly $O(Nn)$.

As the results suggest, the number of returned support vectors will vary 
between the two methods and this difference will depend heavily upon the 
dataset.
The applied optimization complicates things further as the number of merged
support vectors will also vary a great deal between the two methods and
for different datasets.

Solving for the multi-class probabilities given the binary probabilities should
be more efficient with an orthogonal coding matrix. 
Unfortunately the time
spent on this part of the calculation is insignificant for a SVM.
While there don't appear to be significant performance advantages to the method,
at least as applied to SVMs, 
it is very simple and has a certain elegance to it.

