
\section{Introduction}

Many methods of statistical classication can only discriminate between two classes. 
Examples include lineear classifiers such as perceptrons and logistic regression \citep{Michie_etal1994}, 
piecewise linear classifiers \citep{Herman_Yeung1992,Mills2011},
as well as support vector machines \citep{kernel_intro}.
There are many ways of generalizing binary classification to 
multi-class.
Three of the most common are one versus one, one versus the rest and 
error-correcting coding matrices \citep{Hsu_Lin2002}.
Here we are interested in the error-correcting coding matrices
\citep{Dietterich_Bakiri1995, Windeatt_Ghaderi2002} and
rather than use a random coding matrix we are interested in one that is
more carefully designed.

In error-correcting coding, there is a coding matrix, $A$, that specifies
how the set of multiple classes is partitioned.
Typically, the class of the test point is determined by the distance between
a column in the matrix and a vector of binary {\it decision functions}:
\begin{eqnnon}
	c(\vec x) = \arg \min_j | \vec a^{(j)} - \vec r(\vec x) |
	\label{min_dist}
\end{eqnnon}
where $\vec a^{(j)}$ is the $j$th column of the coding matrix and $\vec r$
is a vector of decision functions at {\it test point}, $\vec x$.
If we take the upright brackets as a Euclidean distance, and assume that
each partition partitions all of the classes, that is, there are no zeroes
in $A$, then this reduces to a {\it voting} solution:
\begin{equation}
	c = \arg \max A^T \vec r \label{voting}
\end{equation}
Both \citet{Allwein_etal2000} and \citet{Windeatt_Ghaderi2002} show that to
maximize the accuracy of an error-correcting coding matrix, the distance
between each column, $|\vec a^{(i)} - \vec a^{(j)}|_{i \ne j}$ should be as
large as possible.
Using the same assumptions, this reduces to:
\begin{eqnnon}
	\min |\vec a^{(i)} \cdot a^{(j)}|_{i \ne j}
\end{eqnnon}
In other words, the coding matrix, $A$, should be orthogonal.
This approach to the multi-class problem will be described in detail in this note.

\section{Algorithm}

We wish to design a set of $m$ binary classifiers, each of which return a 
decision function:
\begin{eqnnon}
r_j(\vec x) = P_i(-1 | \vec x) - P_i(+1 | \vec x)
\label{rdef}
\end{eqnnon}
where $P_i(c | \vec x)$ is the conditional probability of the $c$th class of
the $i$th classifier.
Each binary classifier partitions a set of $n$ classes such that for a
given test point, $\vec x$:
\begin{eqnnon}
\sum_j a_{ij} p_j = r_j
\label{multiclass}
\end{eqnnon}
where $A=\lbrace a_{ij} \in \lbrace -1, +1 \rbrace  \rbrace$ is a {\it coding
matrix} and $p_j = p(j | \vec x)$ is the conditional probability of the $j$th
class.
In vector notation:
\begin{equation}
	A \vec p = \vec r \label{inverse}
\end{equation}
The more general case where a class can be excluded, that is the coding 
may include zeroes, $a_{ij} \in \lbrace -1, 0, +1\rbrace$,
will not be addressed here.

Note that this assumes that the binary decision functions, $\vec r$,
estimate the conditional probabilities perfectly.
In practice
there are a set of constrainsts that must be enforced
because $\vec p$ is only allowed to take on certain values.
Thus, we wish to solve the following minimization problem:
\begin{eqnarray}
	\vec p & = & \arg \min_{\vec p} | A \vec p - \vec r | \label{minimization}\\
	\sum_j p_j & = & 1 \label{normalization}\\
	p_j & \ge & 0 \label{nonnegative}
\end{eqnarray}

If $A$ is orthogonal,
\begin{eqnnon}
	A^T A = m I
	\label{orthogonal}
\end{eqnnon}
where $I$ is the $n \times n$ identity matrix and $n$ the number of classes,
then the unconstrained minimization problem is easy to solve. 
Note that the voting solution in (\ref{voting}) is now equivalent to
the inverse solution in (\ref{inverse}).
This allows us to determine the class easily, but we also wish to solve for
the probabilities, $\vec p$, so that none of the constraints in 
(\ref{normalization}) or (\ref{nonnegative}) are violated.
Probabilities are useful for gauging the accuracy of a classification result
when its true value is unknown and for recalibrating an image derived from
statistical classification \citep{Fawcett2006,Mills2009,Mills2011}.

The orthogonality property allows us to reduce the minimization problem 
in (\ref{minimization}) to something much simpler:
\begin{eqnnon}
	\vec p = \arg \min_{\vec p} | \vec p - \vec p_0 |
\end{eqnnon}
where $\vec p_0 = A^T \vec r/m$ with the constraints in (\ref{normalization}) and
(\ref{nonnegative}) remaining the same.
Because the system has been rotated and expanded, the non-negativity 
constraints in (\ref{nonnegative}) remain orthogonal, meaning they are 
independent: enforcing one by setting one of the probabilities to zero, 
$p_k=0$ for example, shouldn't otherwise affect the solution.
This still leaves the normalization constraint in (\ref{normalization}):
the problem, now strictly geometrical, is comprised of finding the point nearest $p_0$ on the diagonal hyper-surface that bisects the unit hyper-cube.

Briefly, we can summarize the algorithm as follows:
1. move to the nearest point that satisfies the normalization constraint,
(\ref{normalization}); 2. if one or more of the probabilities is negative,
move to the nearest point that satisfies both 
the normalization constraint
and the non-negativity constraints, (\ref{nonnegative}), for the negative probabilities;
3. repeat step 2.
More formally, let $\vec 1$ be a vector of all $1$'s:
\begin{itemize}
	\item $i=1$; $n_1=n$
	\item while $\exists k \, p_{ik} < 0 \lor \vec p_i \cdot \vec 1 \ne 1$:
	\begin{itemize}
		\item if $\vec p_i \cdot \vec 1 \ne 1$ then 
		$\vec p_{i+1} = \vec p_i + (\vec p_i \cdot \vec 1 - 1)/n_i$
		\item let $K$ be the set of $k$ such that $p_{i+1,k} < 0$
		\item for each $k$ in $K$:
		\begin{itemize}
			\item $p_k=0$
			\item Remove $k$ from the problem
		\end{itemize}
		\item $n_{i+1}=n_i-|K|$
		\item $i=i+1$
	\end{itemize}
\end{itemize}

Note that resultant direction vectors for each step form an orthogonal set.
For instance, suppose $n_1=4$ and after enforcing the normalization constraint,
the first probability is less than zero, $p_{1,1} < 0$,
then the direction vectors for the two motions are:
\begin{eqnnon}
	\frac{1}{2}[1, 1, 1, 1] \cdot \frac{1}{2\sqrt{3}} [-3, 1, 1, 1] = 0
\end{eqnnon}

More generally, consider the following sequence of vectors:
\begin{eqnnon}
	v_{ij} = \frac{1}{\sqrt{(n-i)^2-2(n-i-1)}} \left \lbrace \begin{array}{rl}
			0; & j < i \\
			-n+i+1; & j=i \\
			1; & j > i
		\end{array} \right .
\end{eqnnon}
where $j \in [1, n]$. \citep{Boyd_Vandenberghe2004}


\section{Constructing the coding matrix}

Finding an $A$ such that $A^T A = m I$ and $a_{ij} \in \lbrace -1, 1, \rbrace$
is quite a difficult combinatorial problem.
Work in signal processing may be of limited applicability because coding
matrices are typically comprised of $0$'s and $1$'s rather than $-1$'s and $+1$'s
as here \citep{Hedayat_etal1999,Panse_etal2014}.
Moreover, there are restrictions required here: 
each row as well as each column must have
both positive and negative elements.

A simple method of designing an orthogonal $A$ is using harmonic series.
Consider the following matrix for eight binary classifiers ($m=8$) and
five classes ($n=5$):
\begin{eqnnon}
	A^T = \left [ \begin{array}{rrrrrrrr}
			-1 & -1 & -1 & -1 & 1 & 1 & 1 & 1 \\
			 1 &  1 & -1 & -1 & -1 & -1 & 1 & 1 \\
			-1 & -1 & 1 & 1 & -1 & -1 & 1 & 1 \\
			-1 & 1 & 1 & -1 & -1 & 1 & 1 & -1 \\
			-1 & 1 & -1 & 1 & -1 & 1 & -1 & 1
	\end{array} \right ]
\end{eqnnon}
This will limit the size of $m$ relative to $n$; more precisely:
$m=2 \log_2 n - 1$.

%Noting which versions of $A$ are degenerate (equivalent) can help us whittle
%away at the problem:
%\begin{itemize}
%	\item $-A$ is equivalent to $A$
%	\item re-arranging either the rows or columns of $A$ makes an equivalent $A$: $T_{ij} A$ or $A T_{ij}^T$ are both equivalent to $A$ where $T_{ij}$ is the elementary matrix which exchanges rows $i$ and $j$
%\end{itemize}

To compute the results in this note, orthogonal coding matrices were generated
using a ``greedy'' algorithm.
We start with an empty matrix.
Candidate vectors are chosen at random to comprise a column of the matrix
but never repeated.
If the candidate vector is orthogonal to existing columns, then it is added to the matrix.
The process is repeated until the matrix is filled.
The matrix is then checked to ensure that each row contains both positive and
negative elements.
A full matrix of $m$ rows is almost always returned but only when $m$
is a multiple of $4$.

Obviously, this method of finding orthogonal coding matrices
will only work for relatively few classes.
More work will need to be done to find efficient methods
of generating these matrices
if they are to be applied to problems with a large number of classes.

\section{Validating the probabilities}

\begin{figure}
	\includegraphics[width=0.9\textwidth]{val_prob.eps}
	\caption{Validating probabilities for the vehicle dataset
		using equations	(\ref{prob_val1}) and (\ref{prob_val2}).
		Probabilities are generated by solving one-versus-one 
		partitioning.}\label{val_prob}
\end{figure}

Since the main goal of the method 
is to solve for the conditional probabilities, 
we need a good way to validate them. 
Here we use a technique based on the
following theorem, which can be derived from Bayes theorem and the 
definition of probability:
\begin{equation}
	\lim_{N \rightarrow \infty} \sum_{i=0}^{N} \left [
	p(c | \vec x_i) - \delta_{cy_i} \right ] = 0
	\label{theorem}
\end{equation}
where 
$\delta$ is the Kronecker delta and
$\lbrace \vec x_i:y_i \rbrace$ is a set of training samples.
To make this useful for real problems we need to do several things.
First, the probability estimates are lumped together and sorted without regard
to the class label, as follows:
\begin{eqnnon}
	p(c_{i+1} | \vec x_{i+1}) \ge p(c_i | \vec x_i)
\end{eqnnon}
Note that in this ordering, there will be $n$ $i$'s indexing the same $\vec x$ 
and the same $y$.
Second, we rearrange Equation (\ref{theorem}) as follows:
\begin{equation}
	N^\prime \approx \sum_{i=i_0}^{N^\prime+i_0} \frac{\delta_{c_i y_i}}{p(c_i | \vec x_i)}
	\label{prob_val1}
\end{equation}
In other words, if the probabilities are well estimated, the RHS should
follow, on average, single step intervals at each step in the trace.
Finally, to prevent singularities at the bottom end, 
we divide it into two parts:
one for all the probabilities greater than a certain threshold, above, 
and one for all those less than this threshold, below:
\begin{equation}
	N^\prime  \approx \sum_{i=N^\prime+i_0}^{i_0} \frac{\delta_{c_i y_i} - 1}{1 - p(c_i | \vec x_i)}
	\label{prob_val2}
\end{equation}
where $i_0$ is the index of the threshold which we choose as the
point between ``winning'' and ``losing'' probabilities:
\begin{eqnnon}
	p(c_i | \vec x_i) \ge 1/n \iff i \ge i_0
\end{eqnnon}
Thus, terms in both Equation (\ref{prob_val1}) and (\ref{prob_val2}) are more
often non-zero than zero.

We use the correlation coefficient to gauge precision and the slope to
measure the bias.
This method of validating conditional probabilities was used in
\citet{Mills2009} for a binary classification problem.
A demonstration is shown in Figure \ref{val_prob}
for the vehicle dataset using the one-versus-one method--see next section.
One weakness of this validation technique is that scores will improve with 
sample size, but for comparative purposes it should be quite adequate.

\section{Results}

\begin{table}
\caption{Speed and skill of multi-class classification results.}\label{class_results}
\input{orthogonal_class_results.tex}
\caption{Precision and bias of solved conditional probabilities.}\label{prob_results}
\input{orthogonal_prob_results.tex}
\end{table}

Orthogonal error correcting codes were tested on six different datasets:
two for digit recognition--``pendigits'' \citep{Alimoglu1996} and
``usps'' \citep{Hull1994}, the space shuttle control dataset--``shuttle''
\citep{King_etal1995}, a satellite land recognition
dataset--``sat'', a similar dataset for image recognition--``segment'',
and a dataset for vehicle recognition--``vehicle'' \citep{Siebert1987}.
The last four are borrowed from the ``statlog'' project \citep{King_etal1995,Michie_etal1994}.

The method was compared with a technique that solves the ``one versus one'' 
partitioning using matrix inversion \citep{Wu_etal2004}.
Both techniques were applied to support vector machines (SVMs) trained using
LIBSVM \citep{Chang_Lin2011}.
Partitions were trained separately then combined by finding the union of
sets of support vectors for each partition.
By indexing into the combined list of support vectors, the algorithms are
optimized in both space and time \citep{Chang_Lin2011}.

Results are shown in Tables \ref{class_results} and \ref{prob_results}.
Each dataset was randomly separated into 70\% training and 30\%
test data.
Confidence limits represent standard deviations over 20 trials.
``U.C'' stands for uncertainty
coefficient, a skill score based on Shannon's channel capacity
\citep{Shannon,Press_etal1992,Mills2011} that has many advantage over simple
fraction of correct guesses or ``accuracy''.
Probabilities are validated in Table \ref{prob_results} with Pearson correlation and slope
applied to a trace generated by 
Equations (\ref{prob_val1}) and (\ref{prob_val2})--see Figure \ref{val_prob}.

The results suggest that there is very little difference between the two
methods, whether in classification accuracy, accuracy of probabily estimates
or classification time.
On the whole, the one-versus-one method appears to have a slight edge in
classification accuracy as well as both precision and bias in probabilty
estimates.
Differences are small but nonetheless statistically significant.
Probability estimates for both methods appear to be highly accurate.

There is the potential for speed increases for binary classification methods
that don't depend on the number of samples in the training data by using
orthogonal coding matrices rather than one-versus-one.
In this case, time efficiency will depend on the number of classes,
$O(n)$, rather than the number of classes squared, $O(n^2)$.
The obvious example is logistic regression, however there will likely be
a concordant decrease in accuracy for the orthogonal coding matrices since
boundaries within partitions will be more complex.

On the other hand, the speed of a SVM is directly proportional to the number 
of support vectors which in turn is proportional, on average,
to the number of training samples: 
$O(N)$, where $N$ is the number of training samples.
Since each partition with an orthogonal coding matrix includes all of the
training data, time efficiency per partition is proportional to the number
of training samples, whereas in one-versus-one, it's proportional, on average,
to the number of training samples divided by the number of classes: $O(N/n)$.
Hence, both methods will have time efficiency roughly $O(Nn)$.

As the results suggest, the number of returned support vectors will vary 
between the two methods and this difference will depend heavily upon the 
dataset.
The applied optimization complicates things further as the number of merged
support vectors will also vary a great deal between the two methods and
for different datasets.

Solving for the multi-class probabilities given the binary probabilities should
be more efficient with an orthogonal coding matrix. 
Unfortunately the time
spent on this part of the calculation is insignificant for a SVM.
While there don't appear to be significant performance advantages to the method
when compared to one-versus-one, at least as applied to SVMs, 
it is very simple and has a certain elegance to it.
It may suggest new directions in the search for more efficient and
accurate methods in multi-class classification.

