Dear Mr. Mills,

We have received the reports from our advisors on your manuscript NEPL-D-18-00535 "Solving for multi-class using orthogonal coding matrices".

With regret, I must inform you that, based on the advice received, the Editors have decided that your manuscript cannot be accepted for publication in Neural Processing Letters.
Below, please find the comments for your perusal.

I would like to thank you very much for forwarding your manuscript to us for consideration and wish you every success in finding an alternative place of publication.

With kind regards,
Dr. Mohamad Hassoun
Editor in Chief
Neural Processing Letters


Comments for the Author:

Reviewer #1: The paper is concerned with solving multi-class classification problems by means of combining binary classifiers via error correcting coding matrices. The paper's main contributions are, first, to construct coding matrices as orthogonal (or rather, orthonormal), and second, to provide a novel scheme to infer a class probability distribution from the error-corrected classification.

The paper's basic ideas are quite neat and attractive in their simplicity. However, the structure and reasoning presented leave much to be desired, such that, in my mind, major revisions are required.

First, the paper does not provide proper motivation. Why are error correcting codes superior to other multiclass variants of binary classification?

R: because they generalize most other methods, including decision trees

Why is a probabilistic interpretation important? What do current probabilistic interpretations (e.g. of SVM outputs) leave to be desired? Instead, the introduction dives directly into the mathematical details without properly introducing the setting. The abstract should also be re-structured to first introduce the problem and _then_ the paper's approach to solving it.

R: the paper is brief because it is intended as a short note or letter, not a full-length paper.

Similarly, the paper fails to introduce the setting properly. As the first formula occurs, we do neither know what a coding matrix is nor do we know what r is. This is needlessly confusing. The paper should be restructured to first explain properly how a coding matrix works and how it relates binary classifications to classes, and then how decoding works. Also, the arguments relating to Euclidean distance (immediately following the decoding equation in the current version) should be moved to the method section because it provides more detail than is needed for an introduction.

R: Again, the paper is intended as a short note, not a full-length paper. There are many papers where error-correcting codes are described in more detail...

In summary, I would strongly recommend to re-write the introduction using the following structure: Setting the scene, motivation (what is the problem and how does the current literature fail to solve it completely?), and then outlining your contribution to solving the problem, on a high level.

Also related to the introduction, the paper appears to not take the current state-of-the-art in terms of error correcting codes fully into account. The newest reference for error correcting codes is from 2002, which is hardly current research. I would strongly recommend to go back into literature research and investigate whether there are more current reviews/contributions. This research could also provide a stronger baseline for comparison compared to random matrices. A quick search on my part revealed, for example, the following contributions:
* Passerini, Pontil, Frasconi, 2004: New results on error correcting output codes of kernel machines, doi:10.1109/TNN.2003.820841
* Escalera, Pujol, and Radeva, 2010: On the Decoding Process in Ternary Error-Correcting Output Codes. doi:10.1109/TPAMI.2008.266
* Rocha and Goldenstein, 2013: Multiclass From Binary: Expanding One-Versus-All, One-Versus-One and ECOC-Based Approaches, doi:10.1109/TNNLS.2013.2274735

R: Are we scientists or fashionistas?

Regarding the method section, the paper glosses over critical points that would help the reader to follow the line of reasoning, namely:
* The maximization of an inner product is not equivalent to the minimization of the Euclidean distance. This only holds if the norm of all vectors is restricted to a constant, i.e. if the norm of all rows of A is the same. This is true in this paper because A is restrictred to entries -1 or 1, but the paper should explain that.

R: Yes.

* For the very same reason, orthogonality only follows from distance maximization under such a normalization constraint. This is a critical restriction because it also means that the paper's key arguments can _not_ be easily translated to the case where the coding matrix may contain zeros, which is typically permitted in the literature. Accordingly, the paper should make this limitation very clear and also discuss how this limitation may affect performance.

R: "The more general case where a class can be excluded, that is the coding may include zeroes, a ij ∈ {−1, 0, +1}, will not be addressed here."

* While equation (2) appears intuitively valid I could not gather from the paper how it is justified. From which reasoning does this follow?

R: Really? I've added a citation to another paper that has this equation...

* Similarly, the paper restricts itself to an Euclidean decoding which, under the constraints imposed here, is equivalent to Hamming decoding. However, Allwein et al. as cited by the paper suggest that Hamming decoding may be inferior to loss-based decoding schemes. The paper should discuss this limitation.


* The equation A^T * A = A * A^T = n*I, again, only holds because of normalization. Otherwise we would obtain a diagonal matrix with non-equal entries which may complicate optimization.

R: Yes.

* The key algorithm of the paper appears not sufficiently justified to me for several reasons. First, it is not immediately clear that the algorithm converges. Second, it is not clear that it finds the optimum. Third, we deal here with a simple, linearly constrained quadratic program which can also be solved via standard QP solvers. Therefore, it is questionable that a custom algorithm is needed at all. The paper should either provide better justification for this custom algorithm or scrap it and replace it by a QP solver.

R: Yes, it might be nice to have a more rigorous proof of both convergence and validity, however it seems well enough justified just on heuristics: since the contraints are orthogonal and the metric isotropic, enforcing one contraint shouldn't otherwise alter the solution. Likewise, if we approach the normalization contraint from an unconstrained minimum in an orthogonal direction, this is the shortest distance and therefore the new minimum. This is explained in the paper.

* If the algorithm is kept, I would also suggest to provide a small example to see how it works. The current example only demonstrates orthogonality of directions and it is not clear why this orthogonality is helpful.
* At the same position the author provides some sequence of vectors defined by v_ij but I could not determine what this sequence is supposed to mean in the context of the paper or the further argument.

R: I'm not certain why they are important, but they are...

The experiments are generally well-done and properly support the claims of the paper. However, the baseline provided (random error correcting code matrices) appears somewhat weak and thus I woudl suggest to consider a stronger baseline from the literature. Also, the paper makes claims regarding significance but does not provide statistical test results to back those up. If the paper could add 2-3 more datasets, a statistical analysis would be possible, e.g. using a wilcoxon sign-rank test.

R: Not sure a more rigorous test is worth it. I chose random error-correcting codes because this is comparing apple to apples: it shows that the method does what it should do. It is not, however, better than the 1-vs.-1 method. It's slightly worse. Also in regards to significance: Z-test. (difference in means divided by the root of the sum of the variances should be less than about 2)

Finally, the reference list is badly designed. In almost all cases, paper titles are missing such that finding the references is needlessly complicated. Using DOIs whenever possible would also be much appreciated.

R: I just used the Latex classes suppled by Springer.

In summary, I believe that, at this point, the paper has serious flaws in its introduction, as well as its theoretical and algorithmic side, but is not beyond repair. Indeed, I believe that the core ideas provided are a valid and helpful contribution which is interesting to the community.


Reviewer #2: Peter Mills
Solving for multi-class using orthogonal coding matrices

In my opinion, the author fails to motivate and explain his work in an appropriate manner.

The presentation is rather brief and difficult to follow. Notation is also quite unclear.
As just one example, the unnumbered Equation before Eq. (1) contains P_j(-1|x) and P_j(+1|x),
with in the following line explaining that P_j(c|x) is the cond. prob. of the c-th class.

A comparison with results for randomized coding matrices is given with little to no explanation
of why this is relevant, what other methods might exist etc.

All in all, this work might deserve publication, but in its current form the presentation lacks
clarity. Moreover, the work should be better motivated and put into perspective of the field.



Reviewer #3: Solving for multi-class using orthogonal coding matrices

comments:
- first:
         - do you think the title sounds right?
         - why 'we' if you are only one?

R: It's either that or the passive voice. It's well known how much the grammarians hate the passive voice...

         - the paper is in parts a bit sloppy and
           more like a technical report ('constrainsts' - aspell?)
         - please take also more time / effort on the overall presentation
           of the paper - if you are not willing to take care of the form
           I am not sure how much precision can be expected in the main content
- second:
        - at the current level I think this paper could be
          a better fit to a conference rather a journal
           The proposal is rather minor and not yet fully developed

R: Are you going to pay for it?

- it maybe good to come up a bit with related work on
  error correcting output code (ecoc) and how the matrices/codes
  are generated

R: Again, this is not meant to be full-length article. I have written another paper that has all of that stuff but it's been even more poorly received than this one...

- please provide a more elaborated motivation why you focus on ecoc
  and believe it is important and the current state of the art is insufficient
- it may also not harm to introduce the ecoc after the introduction
  part in its classical form to give the reader a better view
  (actually ecocs are not so popular and most people have never heard
  about it and may find it strange - hard to believe)
- '...will not be addressed here' - hm, thats bad because it is often
  nice to be able to ignore wide parts of the data set in calculating
  individual models
- 'for recalibrating an image derived from statistical classification'
  --? image??
- p.3. there are nice packages to describe algorithms in Latex please
  use one of them

R: Tried one, the results really weren't any better than what I have here...

- sec 2 keeps me puzzled what are actually given / known entities - from
  the description it looks a bit like everything is unknown and you are
  optimizing - well what actually? - I think the notation is not right
- you should also motivate why we need all this - if I remember right the
  story was to measure hamming distances between the predicted output codes
  to get a final decision
- in particular if I got it right your algorithm is more or less a heuristic?

R: Yes that's correct. The fact that it also happens to work is besides the point...

- are there any possible guarantees you can give - if not I may suggest
  to provide a more in depth analysis on a much larger set of challenging
  datasets (real multiclass)
- 'shuttle' may show some significant difference
  in your measures between orth ECOC / random ECOC but for the other I
  can not see a strong effect
- if your focus is on runtime performance I guess you may run into problems
  because there are many fast classifiers (e.g. core vector machines)
  and SVM (in particular libsvm) is not a good starting point
- which parameters / kernel is used for the libSVM?
- table 1 please avoid more than 2 decimal places - your timing
  values basically show it is quick ;-)
- although you like U.C. and Brier many other dont, so please make your
  prediction also crisp and provide some simple mean/std-dev in the accuracy
- add some results if e.g. I use 1 vs all, each against each classification
  on the same data to have an estimate if your ecoc approach is not having
  a negative impact - or how much does it have a positive impact - considering
  the efforts behind it
- in CS we like to have also titles in the references, full pages range
- literature is not very much upto date - please include (more) material
  of the last 10 years around the considered subject (ecoc, ecoc code matrices, applications of ecoc ...)
- there are no NEPL papers in your list - do you think your paper fits to NEPL?



