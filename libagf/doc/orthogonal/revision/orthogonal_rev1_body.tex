
\section{Introduction}

Many methods of statistical classication can only discriminate between two classes. 
Examples include lineear classifiers such as perceptrons and logistic regression \citep{Michie_etal1994}, 
piecewise linear classifiers \citep{Herman_Yeung1992,Mills2018},
as well as support vector machines \citep{kernel_intro}.
There are many ways of generalizing binary classification to 
multi-class.

One should distinguish between multi-class methods that only use a subset
of the binary classifiers, adding more as the algorithm narrows down the
class, and those that use all of the binary classifiers, combining the results
or solving for the class probabilities.
In the former category, we have hierarchical multi-class classifiers 
such as decision trees \citep{Cheong_etal2004,Lee_Oh2003} and decision
directed acyclic graphs (DDACs) \citep{Platt_etal2000}.
In the latter category,
two common methods are one versus one and one versus the rest \citep{Hsu_Lin2002}
These in turn generalize to error-correcting codes (ECC) \citep{Dietterich_Bakiri1995}.

The first application of error-correcting codes used random codes:
the assumption is that if there are enough codes (binary classifiers)
they will adequately span the classes.
Later work focused on optimizing the design of the codes: what type of
codes will best span the classes and produce the most accurate results?
Here we can also distinguish between two type: those that use the data
to help design the codes \citep{Crammer_Singer2002,Zhou_etal2008,Zhong_Cheriet2013}
and those that are independent of the data but use the mathematical 
properties of the codes to aid in the design \citep{Allwein_etal2000,Windeatt_Ghaderi2002,Zhou_etal2019}.
It is these latter type of optimized error-correcting codes we are interested in here.

In error-correcting coding, there is a coding matrix, $A$, that specifies
how the set of multiple classes is partitioned for each binary classifier.
For a given column, 
if members of the $j$th class are to take on a $-1/+1$ for the
binary classifier, then the $j$th row is assigned a $-1/+1$.
If the $j$th class is left out, then the $j$th row is assigned a $0$.
Typically, the class of the test point is determined by the distance between
a row in the matrix and a vector of binary {\it decision functions}:
\begin{equation}
	c(\vec x) = \arg \min_i | \vec a_i - \vec r(\vec x) |
	\label{min_dist}
\end{equation}
where $\vec a_i\in \lbrace -1,0,+1 \rbrace$ 
is the $i$th row of the coding matrix and 
$\vec r$ is a vector of decision functions at {\it test point}, $\vec x$.
If we take the upright brackets as a Euclidean distance and expand:
\begin{eqnnon}
	c = \arg \min_i \sum_j \left ( | \vec a_i | + | \vec r | - 2 \vec a_i  \cdot \vec r_i  \right ) 
\end{eqnnon}
Since $| \vec r |$ is constant over $i$, it may be removed from the expression.
Also, for the purposes of this note,
each row of $A$ has the same number of non-zero entries, hence:
\begin{eqnnon}
	| \vec a_i | = | \vec a_j | = const.
\end{eqnnon}
This is most evident for the case in which each binary classifier partitions
all of the classes so that there are no zeros in $A$
as is the case for the one-versus-the-rest partitioning.
Then (\ref{min_dist}) reduces to a {\it voting} solution:
\begin{equation}
	c = \arg \max A \vec r \label{voting}
\end{equation}
Both \citet{Allwein_etal2000} and \citet{Windeatt_Ghaderi2002} show that to
maximize the accuracy of an error-correcting coding matrix, the distance
between each row, $|\vec a_i - \vec a_j|_{i \ne j}$ should be maximized.
Using the same assumptions, this reduces to:
\begin{eqnnon}
	\min |\vec a_i \cdot \vec a_j|_{i \ne j}
\end{eqnnon}
Note the absolute value prevents degenerate rows.
In other words, the coding matrix, $A$, should be orthogonal.

In this note, we describe a fast and simple algorithm that uses orthogonal coding matrices to solve for the conditional probabilites in multi-class
classification.
There are three reasons to require the conditional probabilities:
\begin{enumerate}
	\item Probabilities provide useful extra information, specifically how accurate a given classification is in absence of knowledge of its true value.
	\item The relationship between the binary probabilities and the
		multi-class probabilities derives uniquely and rigorously from probability theory.
	\item Binary classifier that do not return calibrated probability estimates, but nonetheless supply a continuous decision function are easy to recalibrate so that the decision function more closely resembles a probability \citep{Jolliffe_Stephenson2003,Platt1999}.
\end{enumerate}
Two types of orthogonal ECCs with along three other multi-class 
methods--1 vs. 1, 1 vs. the rest, and random ECCs--will be tested on seven
different datasets to see how they compare in terms of classification speed,
classification accuracy and accuracy of the conditional probabilities.

\section{Algorithm}

We wish to design a set of $n$ binary classifiers, each of which return a 
decision function:
\begin{eqnnon}
r_j(\vec x) = P_j(-1 | \vec x) - P_j(+1 | \vec x)
\label{rdef}
\end{eqnnon}
where $P_j(c | \vec x)$ is the conditional probability of the $c$th class of
the $j$th classifier.
Each binary classifier partitions a set of $m$ classes such that for a
given test point, $\vec x$:
\begin{eqnnon}
	\sum_{i=1}^m a_{ij} p_i = r_j; ~~~ j=[1..n]
\label{multiclass}
\end{eqnnon}
where $A=\lbrace a_{ij} \in \lbrace -1, +1 \rbrace  \rbrace$ is a {\it coding
matrix} and $p_i = p(i | \vec x)$ is the 
conditional  probability of the $i$th class.
In vector notation:
\begin{equation}
	A^T \vec p = \vec r \label{inverse}
\end{equation}
This result derives from the fact that the class probabilities are
additive \citep{Kong_Dietterich1997}.
The more general case where a class can be excluded, that is the coding 
may include zeroes, $a_{ij} \in \lbrace -1, 0, +1\rbrace$,
will not be addressed here. 

Note that this assumes that the binary decision functions, $\vec r$,
estimate the conditional probabilities perfectly.
In practice
there are a set of constraints that must be enforced
because $\vec p$ is only allowed to take on certain values.
Thus, we wish to solve the following minimization problem:
\begin{equation}
	\arg \min_{\vec p} | A^T \vec p - \vec r | \label{minimization}
\end{equation}
\begin{eqnarray}
	\sum_{i=1}^m p_i & = & 1 \label{normalization}\\
	p_i & \ge & 0; ~~~ i=[1..m] \label{nonnegative}
\end{eqnarray}

If $A$ is orthogonal,
\begin{eqnnon}
	A A^T = n I
	\label{orthogonal}
\end{eqnnon}
where $I$ is the $m \times m$ identity matrix,
then the unconstrained minimization problem is easy to solve. 
Note that the voting solution in (\ref{voting}) is now equivalent to
the inverse solution in (\ref{inverse}).
%and also that, unless coding matrix, $A$, is 
%square, $n=m$, it is only orthogonal in one direction.
This allows us to determine the class easily, but we also wish to solve for
the probabilities, $\vec p$, so that none of the constraints in 
(\ref{normalization}) or (\ref{nonnegative}) are violated.
Probabilities are useful for gauging the accuracy of a classification result
when its true value is unknown and for recalibrating an image derived from
statistical classification \citep{Fawcett2006,Mills2009,Mills2011}.

The orthogonality property allows us to reduce the minimization problem 
in (\ref{minimization}) to something much simpler:
\begin{eqnnon}
	\arg \min_{\vec p} | \vec p - \vec p_0 |
\end{eqnnon}
where $\vec p_0 = A \vec r/n$ with the constraints in (\ref{normalization}) and
(\ref{nonnegative}) remaining the same.
Because the system has been rotated and expanded, the non-negativity 
constraints in (\ref{nonnegative}) remain orthogonal, meaning they are 
independent: enforcing one by setting one of the probabilities to zero, 
$p_k=0$ for example, shouldn't otherwise affect the solution.
This still leaves the normalization constraint in (\ref{normalization}):
the problem, now strictly geometrical, is comprised of finding the point nearest $p_0$ on the diagonal hyper-surface that bisects the unit hyper-cube.

Briefly, we can summarize the algorithm as follows:
1. move to the nearest point that satisfies the normalization constraint,
(\ref{normalization}); 2. if one or more of the probabilities is negative,
move to the nearest point that satisfies both 
the normalization constraint
and the non-negativity constraints, (\ref{nonnegative}), for the negative probabilities;
3. repeat step 2.
More formally, let $\vec 1$ be a vector of all $1$'s:
\begin{itemize}
	\item $i:=0$; $m_0:=m$
	\item while $\exists k \, p_{ik} < 0 \lor \vec p_i \cdot \vec 1 \ne 1$:
	\begin{itemize}
		\item if $\vec p_i \cdot \vec 1 \ne 1$ then 
		$\vec p_{i+1} := \vec p_i + (\vec p_i \cdot \vec 1 - 1)/m_i$
		\item let $K$ be the set of $k$ such that $p_{i+1,k} < 0$
		\item for each $k \in K$:
		\begin{itemize}
			\item $p_k:=0$
			\item Remove $k$ from the problem
		\end{itemize}
		\item $m_{i+1}:=m_i-|K|$
		\item $i:=i+1$
	\end{itemize}
\end{itemize}

%\input{orthogonal_algorithm.tex}

Note that resultant direction vectors for each step form an orthogonal set.
For instance, suppose $m_0=4$ and after enforcing the normalization constraint,
the first probability is less than zero, $p_{1,1} < 0$,
then the direction vectors for the two motions are:
\begin{eqnnon}
	\frac{1}{2}[1, 1, 1, 1] \cdot \frac{1}{2\sqrt{3}} [-3, 1, 1, 1] = 0
\end{eqnnon}

More generally, consider the following sequence of vectors:
\begin{eqnnon}
	v_{ij} = \frac{1}{\sqrt{(m-i)^2-2(m-i-1)}} \left \lbrace \begin{array}{rl}
			0; & j < i \\
			-m+i+1; & j=i \\
			1; & j > i
		\end{array} \right .
\end{eqnnon}
where $i \in [1, m]$ and $j \in [1, m]$. \citep{Boyd_Vandenberghe2004}
A nice feature of this method, in addition to being fast,
is that it is divided into two stages: a solution stage and a normalization stage.

\section{Constructing the coding matrix}

\label{construction}

Finding an $A$ such that $A A^T = n I$ and $a_{ij} \in \lbrace -1, 1, \rbrace$
is quite a difficult combinatorial problem.
Work in signal processing may be of limited applicability because coding
matrices are typically comprised of $0$'s and $1$'s 
rather than $-1$'s and $+1$'s \citep{Hedayat_etal1999,Panse_etal2014}.
A further restriction is that
columns must have both positive and negative elements, or:
\begin{equation}
	\left | \sum_{i=0}^m a_{ij} \right | < m;  ~~~ i=[1..n] \label{restriction}
\end{equation}

A simple method of designing an orthogonal $A$ is using harmonic series.
Consider the following matrix for six classes ($m=6$) 
and eight binary classifiers ($n=8$):
\begin{equation}
	A = \left [ \begin{array}{rrrrrrrr}
			 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 \\
			-1 & -1 & -1 & -1 & 1 & 1 & 1 & 1 \\
			-1 & -1 & 1 & 1 & -1 & -1 & 1 & 1 \\
			-1 & 1 & -1 & 1 & -1 & 1 & -1 & 1 \\
			 1 &  1 & -1 & -1 & -1 & -1 & 1 & 1 \\
			-1 & 1 & 1 & -1 & -1 & 1 & 1 & -1 
	\end{array} \right ]
	\label{harmonic}
\end{equation}
This will limit the size of $m$ relative to $n$; more precisely:
$m \le \lfloor 2 \log_2 n \rfloor$. Moreover, only certain values of $n$
will be admitted: $n=2^t$ where $t$ is a whole number.

The first three rows in (\ref{harmonic}) comprise a Walsh-Hadamard code \citep{Arora_Barak2009}:
all possible permutations are listed.
A square ($n=m$) orthogonal coding matrix is called a Hadamard matrix
\citep{Sylvester1867}.
It can be shown that besides $n=1$ and $n=2$, only Hadamard matrices of size
$n=4t$ exist,  
and it is still unproven that examples exist for all values of $t$
\citep{Hedayat_Wallis1978}.
A very simple, recursive method exists to generate matrices of size $n=t^2$ 
\citep{Hedayat_Wallis1978} but cannot be made to have the property in (\ref{restriction}): 
the matrix includes a both a row and column of only ones.
Such a matrix will include a ``harmonic series'' of the same type as in
(\ref{harmonic}).

%Noting which versions of $A$ are degenerate (equivalent) can help us whittle
%away at the problem:
%\begin{itemize}
%	\item $-A$ is equivalent to $A$
%	\item re-arranging either the rows or columns of $A$ makes an equivalent $A$: $T_{ij} A$ or $A T_{ij}^T$ are both equivalent to $A$ where $T_{ij}$ is the elementary matrix which exchanges rows $i$ and $j$
%\end{itemize}

To compute the results in this note, orthogonal coding matrices were generated
using a ``greedy'' algorithm.
We choose $n$ to be the smallest multiple of $4$ equal to or larger than $m$.
and start with an empty matrix.
Candidate vectors containing both positive and negative elements 
are chosen at random to comprise a row of the matrix but never repeated.
If the candidate vector is orthogonal to existing rows, then it is added to the matrix.
New candidates are tested until the matrix is filled or we run out of permutations.
A full matrix is almost always returned especially if $m<n$.
The matrix is then checked to ensure that 
each column contains both positive and negative elements.
Note that the whole process can be repeated as many times as necessary.

More work will need to be done to find efficient methods
of generating these matrices
if they are to be efficiently applied to problems with a large number of classes.


\section{Results}

\begin{table*}
\caption{Solution time, uncertainty coefficient and Brier score for six different datasets using five different coding matrices: 1 vs. 1, 1 vs. the rest, random error correcting codes, orthogonal "strict" coding, and orthogonal "non-strict" coding. Logistic regression is used as the base binary classifier.}\label{class_results_lin}
\input{results_lin2019_09_06.tex}
\end{table*}

\begin{table*}
\caption{Solution time, uncertainty coefficient and Brier score for six different datasets using five different coding matrices: 1 vs. 1, 1 vs. the rest, random error correcting codes, orthogonal "strict" coding, and orthogonal "non-strict" coding. A support vector machine is used as the base binary classifier.}\label{class_results_svm}
\input{results_svm2019_09_06.tex}
\end{table*}

\begin{table*}
\caption{Solution time, uncertainty coefficient and Brier score for six different datasets using five different coding matrices: 1 vs. 1, 1 vs. the rest, random error correcting codes, orthogonal "strict" coding, and orthogonal "non-strict" coding. A piecewise linear classifier is used as the base binary classifier.}\label{class_results_acc}
\input{results_acc2019_09_09.tex}
\end{table*}

Orthogonal error-correcting codes were tested on six different datasets:
two for digit recognition--``pendigits'' \citep{Alimoglu1996} and
``usps'' \citep{Hull1994}; the space shuttle control dataset--``shuttle''
\citep{King_etal1995}; a satellite land recognition
dataset--``sat''; a similar dataset for image recognition--``segment'';
and a dataset for vehicle recognition--``vehicle'' \citep{Siebert1987}.
The last four are borrowed from the ``statlog'' project \citep{King_etal1995,Michie_etal1994}.

The method was compared with random error-correcting codes using the same
number of codes or matrix rows, $m$.
Random codes were solved using a constrained linear least squares method based
on the Karesh-Kuhn-Tucker conditions \citep{Lawson_Hanson1995}.
Both techniques were applied to support vector machines (SVMs) trained using
LIBSVM \citep{Chang_Lin2011}.
Partitions were trained separately then combined by finding the union of
sets of support vectors for each partition.
By indexing into the combined list of support vectors, the algorithms are
optimized in both space and time \citep{Chang_Lin2011}.

The same parameters were used for both the random and orthogonal codes and 
for all partitions.
All datasets were trained using  ``radial basis function'' (Gaussian)
kernels of differing widths.

Results are shown in Table \ref{class_results}.
Confidence limits represent standard deviations over 20 trials using
different, randomly chosen coding matrices.
For each trial, datasets were randomly separated into 70\% training and 30\%
test.
``U.C'' stands for uncertainty
coefficient, a skill score based on Shannon's channel capacity
\citep{Shannon,Press_etal1992,Mills2011} that has many advantage over simple
fraction of correct guesses or ``accuracy''.
Probabilities are validated with the Brier score 
which is root-mean-square
error measured against the truth of the class as a 0 or 1 value
\citep{Brier1950,Jolliffe_Stephenson2003}.

For all of the datasets tested, orthogonal coding matrices provide a small but
significant improvement over random coding matrices in both classification
accuracy and in the accuracy of the conditional probabilities.
This is in line with the literature as in \citet{Dietterich_Bakiri1995,Windeatt_Ghaderi2002}.
Results are also more consistent for the orthogonal codes as given by the
calculated error bars.

Also as expected, solution times are
considerably faster for the orthogonal coding matrices.
Depending on the problem and classification method, this may or may not
be significant.
Since SVM is a relatively slow classifier, solution times
are a minor portion of the total.
For fast classifiers such as a linear classifier or perceptron,
solving the constrained optimization problem for the probabilities
could easily comprise the bulk of classification times.
Note that for both methods under examination, time to perform the binary
classifications should be roughly equivalent.

As predicted, solving for multi-class using orthogonal coding matrices 
is more accurate than the equivalent problem using random coding matrices.
While the difference is small,
the method is simple and elegant and
may suggest new directions in the search for more efficient and
accurate multi-class classification algorithms.

