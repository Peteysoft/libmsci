\documentclass{article}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage[dvips]{graphicx}

\usepackage{natbib}
\usepackage{url}

\usepackage{boxedminipage}

\bibliographystyle{apa}

\newenvironment{eqnnon}{\begin{equation*}}{\end{equation*}}

\newenvironment{eqnarraynon}{\begin{eqnarray*}}{\end{eqnarray*}}

\begin{document}


\title{Solving for multi-class: a survey and synthesis}

\author{Peter Mills\\\textit{peteymills@hotmail.com}}

\maketitle

\section*{Abstract}

We review the most common methods of solving for multi-class from binary
and generalize them to a common framework.
Since conditional probabilties are useful both for quantifying the accuracy of
an estimate and for calibration purposes, 
these are a required part of the solution.
There is some indication that the best solution for multi-class classification
is dependent on the particular dataset.
As such, we are particularly interested in data-driven solution design, 
whether based on a priori considerations or empirical examination of the data.
Numerical results indicate that while
a one-size-fits-all solution consisting of one-versus-one 
is appropriate for most datasets,
a minority will benefit from a more customized approach.
The techniques discussed in this paper allow for a large variety of multi-class
configurations and solution methods to be tested so as to optimize 
classification accuracy, accuracy of conditional probabilities and speed.

\tableofcontents

\section{Introduction}

Many statistical classifiers can only discriminate between two classes.
Common examples include linear classifiers such as perceptrons and
logistic regression classifiers \citep{Michie_etal1994} as well as extensions
of these methods such as support vector machines (SVM) \citep{Mueller_etal2001} and 
piecewise linear classifiers \citep{Mills2018, Bagirov2005}.
There are many possible ways of extending a binary classifier to deal
with multi-class classification and the options increase exponentially
with the class labels.
Moreover, the best method may well depend on the type of problem
\citep{Dietterich_Bakiri1995,Allwein_etal2000}.

The goal of this paper is not only to provide a summary of the
best methods of solving for multiclass, but to synthesize
these ideas into a comprehensive framework whereby a multi-class problem
can be solved using a broad array of configurations for the binary
classifiers. 
In particular, we require that any algorithm solve 
for the multi-class conditional probabilities.
These are useful both for gauging the accuracy of a result and for various
forms of recalibration \citep{Jolliffe_Stephenson2003,Fawcett2006,Mills2009,Mills2011}.

\subsection{Definition of the problem}

\label{description}

In a statistical classication problem we are given a set of ordered pairs, 
$\lbrace \vec x_j : y_j \rbrace$, of {\it training data},
where the vector, $\vec x_j$, is the location of the {\it sample} in 
the {\it feature space},
$y_j \in [1..n_c]$ is the class of the sample,
$n_c$ is the number of classes,
and the classes are
distributed according to an unknown conditional distribution,
$P(c | \vec x)$ with $c \in [1..n_c]$ the class label and $\vec x$ the location
in feature space.

Given an arbitrary {\it test point}, $\vec x$, 
we wish to estimate $P(c | \vec x)$, however we only have
the means to estimate some binary component of it, that is we have a 
set of binary classifiers, 
each returning a {\it decision function}, $r_i(\vec x)$.
In this paper we assume that the decision function 
returns estimates of the difference in conditional probabilities:
\begin{eqnnon}
	r_i(\vec x) \approx p_i(+1|\vec x) - p_i(-1|\vec x)
\end{eqnnon}
where $p_i(c|\vec x)$ is the conditional probability of the $i$th
binary classifier.
The decision function is trained using the same type of ordered pairs as
above except that the classes can only take on one of two values,
which for convenience are chosen as either $-1$ or $+1$, 
that is, $y_{ij} \in \lbrace -1, +1 \rbrace$.

The problem under consideration in this review is, first,
how can we partition the classes, $\lbrace y_i \rbrace$, for each binary
classifier?
That is we want to create a mapping of the form:
\begin{equation}
	y_{ij} (y_j) = \left \lbrace  \begin{array}{lr}
-1 & | y_j \in C_i^+ \\
+1 & | y_j \in C_i^-
\end{array}
	\right . \label{mapping}
\end{equation}
where $y_{ij}$ is the class value of $j$th sample of the transformed data 
for trainng the $i$th binary classifier and 
$C_i^+ \subset \lbrace 1..n_c \rbrace$ is the set of class labels from the original set,
$\lbrace 1..n_c \rbrace$, that map to $+1$ while
$C_i^- \subset \lbrace 1..n_c \rbrace$ is the set of classes that map to $-1$.

And second, once we have partitioned the classes
and trained the binary classifers,
how do we solve for the multi-class conditional probabilities, $P(c|\vec x)$?
The class of the test point may be estimated through maximum likelihood:
\begin{equation}
	c(\vec x)=\arg \max_i P(i | \vec x)
	\label{maximum_likelihood}
\end{equation}

%given a test point, $\vec x$,
%or alternatively, directly for $P(c | \vec x)$ where $c$ is either the most likely class
%or else can be any of the classes, $c \in [1,n_c]$?
%--that is all of the conditional probabilities 
%or just that of the ``winning'' class?


\section{Nonhierarchical multi-class classification}

In {\it non-hierarchical} multi-class classification, we solve for the
classes or probabilities of the multi-class problem all at once:
all the binary classifiers are used in the solution and the result of
one binary classifier does not determine the use of any of the others.
Using the notation provided in Section \ref{description}, 
we can write a system of equations relating
the multi-class conditional probabilities to the decision
functions:
\begin{equation}
	r_i(\vec x) = \frac{\sum_{j=1}^{n_i^+} P(c_{ij}^+|\vec x) - \sum_{j=1}^{n_{i}^-} P(c_{ij}^-|\vec x)}{\sum_{j=1}^{n_i^-} P(c_{ij}^-|\vec x) + \sum_{j=1}^{n_i^+} P(c_{ij}^+|\vec x)}
	\label{decision_function}
\end{equation}
where 
$c_{ij}^- \in C_i^-$,
$c_{ij}^+ \in C_i^+$,
$n_i^-$ is the number of class labels on the negative side of
the $i$th partition,
and $n_i^+$ is the number of class labels on the positive side of
the $i$th partition

It's more natural (and considerably simpler) to describe the problem using a
{\it coding matrix}, $A$, 
which is structured such that
$a_{ij} \in \lbrace -1, 0, 1 \rbrace$, 
where $i$ enumerates the binary classifier and
$j$ enumerates the class of the multi-class problem.
In other words, if $a_{ij}$ is $-1$/$+1$, we would assign each of the $j$th class
labels in the training data a value of $-1$/$+1$ when training the $i$th
binary classifier. If the value is $0$, the $j$th class label is excluded.
\citep{Dietterich_Bakiri1995,Windeatt_Ghaderi2002}

The non-zero elements of $A$ are:
\begin{eqnarraynon}
	a_{ic_{ik}^-} & = & -1 | ~k = [1..n_{c_i^-}]\\
a_{ic_{ik}^+} & = & +1 | ~k=[1..n_{c_i^+}]
\end{eqnarraynon}

%We can relate the multi-class probabilities to the output of the 
%binary classifiers as follows:
We can rewrite Equation (\ref{decision_function}) using the coding 
matrix as follows:
\begin{equation}
	\frac{\sum_{j=1}^{n_c} a_{ij} p_j}{\sum_{j=1}^{n_c} |a_{ij}| p_j} = r_i
	\label{non_hier}
\end{equation}
where $\vec p=\lbrace p_i | i=[1..n_c]\rbrace$, 
is a vector of multi-class conditional probabilities, $p_i=P(i|\vec x)$, 
$n_c$ is the number of classes,
$\vec r=\lbrace r_i| i=[1..n_p]\rbrace$ 
is the vector of decision functions,
and
$n_p$ is the number of partitions.

Some rearrangement shows that
we can solve for the probabilities, $\vec p$, via matrix inversion:
\begin{eqnarray}
	Q \vec p & = & \vec r \label{basic_system}\\
	%q_{ij} & = & a_{ij} + \delta(a_{ij}) r_i \\
	q_{ij} & = & a_{ij} + (1-|a_{ij}|) r_i 
	\label{matrix_equation2}
\end{eqnarray}
%and $\delta$ is the Kronecker delta.
Note that $Q$ reduces to $A$ if $A$ contains no zeroes \citep{Kong_Dietterich1997}.
The case of a coding matrix that contains no zeroes, that is all the partitions divide up all the
classes rather than a subset, will be called the {\it strict} case.
From a computational perspective, in the strict case,
$Q$ must be regenerated for every new test point or value of $\vec r$ 
whereas in the non-strict case, $A$ can be inverted or decomposed and then
applied to every subsequent value of $\vec r$.

Because the decision functions, $\vec r$, are not estimated perfectly,
the final probabilities may need to be constrained and the inverse
problem solved via minimization:
\begin{equation}
	\vec p = \arg \min_{\vec v} | Q \vec v - \vec r | \label{minimization_problem}
\end{equation}
subject to:
\begin{eqnarray}
	\sum_{i=1}^{n_c} p_i & = & 1 \label{normalization}\\
	\vec p & \ge & \vec 0 \label{nonnegative}
\end{eqnarray}
where straight brackets, $||$, denotes a vector norm which  
in this case is the Euclidian or $L^2$ norm and
$\vec 0$ is a vector of all zeroes.

\subsection{Basic inverse solution}

Equation (\ref{minimization_problem}) can be solved via the normal
equation:
\begin{equation}
	Q^T Q \vec p = Q^T \vec r
	\label{normal_equation}
\end{equation}
Because the binary probability estimates in $\vec r$ are rarely perfect, however,
in many cases the constraints in (\ref{normalization}) and (\ref{nonnegative}) will not be satisfied. 
Therefore, for most applications, either the results will need to be adjusted, likely reducing accuracy, or the problem constrained.

It is straightforward to incorporate the normalization constraint in (\ref{normalization}) into the problem. 
There are at several ways of doing this. The most
obvious is to write one probability in terms of the others:
\begin{equation}
	p_k = 1 - \sum_{i|i \ne k} p_i
	\label{reduce_dimension1}
\end{equation}
And solve the following, reduced dimensional linear system:
\begin{equation}
	\sum_j (q_{ij} - q_{ik} ) p_j = r_i - q_{ik}
	\label{reduce_dimension2}
\end{equation}
A more symmetric method is the Lagrange multiplier which will be
derived in Section \ref{one_vs_one}.
\citet{Lawson_Hanson1995} discuss at least two other methods of
enforcing equality constraints on linear least squares problems.
Since they are inequality constraints, those in (\ref{nonnegative}) are
harder to enforce and details  will be left to a later section.

\subsection{Voting solution}

In many other texts \citep{Allwein_etal2000, Hsu_Lin2002, Dietterich_Bakiri1995},
the class of the test point is determined by how close $\vec r$
is to each of the columns in $A$:
\begin{eqnnon}
	c = \arg \min_i |\vec a^{(i)} - \vec r|^2
\end{eqnnon}
where $\vec a^{(i)}$ is the $i$th column of $A$.
For the norm, $||$, Hamming distance is
frequently used, which is the number of bits that must be changed
in a binary number in order for it to match another binary number.
This assumes that each decision function returns only one of two values: 
$r_i \in \lbrace -1, +1 \rbrace$.
If the coding matrix is strict, then:
\begin{eqnnon}
	|\vec a^{(j)} - \vec r| = \sum_i \delta_{a_{ij} r_i}
\end{eqnnon}
where $\delta$ is the Kronecker delta.
\citet{Allwein_etal2000} 
tailor the matric on the basis of the binary classifier used, each of which
will return a different type of continuous decision function 
(that doesn't represent the difference in conditional probabilities).

Here we are assuming that the decision functions return an approximation of the 
difference in conditional probabilities of the binary classifier.
In this case a more natural choice of metric is the Euclidian. Expanding:
\begin{eqnnon}
	c = \arg \min_i \left \lbrace |\vec a^{(i)}|^2 - 2 \vec a^{(i)} \cdot \vec r + |\vec r|^2 \right \rbrace
\end{eqnnon}
The length of $\vec r$ is independent of $i$, hence it can be eliminated from the expression.
For the strict case, the length of each column will also be constant at $|\vec a^{(i)}|=\sqrt{n_p}$.
Even for the non-strict case, we would expect the column lengths to be close for typical coding 
matricesr; for instance, the column lengths are equal in the one-versus-one case.
Eliminating these two terms produces a {\it voting} solution:
\begin{eqnnon}
	c = \arg \max A^T \vec r
\end{eqnnon}
That is, if the sign of $r_i$ matches the $i$th element of the column, then a vote is cast 
in proportion to the size of $r_i$ for the class label corresponding to the column number.

A voting solution can be used for any coding matrix and 
is especially appropriate if each $r_i$ returns only $-1$ or $+1$.  
The LIBSVM libary, for instance, uses a one-versus-one arrangement with a voting
solution if probabilities are not required \citep{Chang_Lin2011}.
The disadvantage of a voting solution is
that it does not return estimates of the probabilities.

\section{Common coding matrices}

There are a number of standard, symmetric coding matrices that are commonly used
to solve for multi-class.
These include ``one-versus-the-rest'', ``one-versus-one'',
as well as error-correcting coding matrices such as orthogonal and
random.
We discuss each of these in turn and
use them to demonstrate how to solve for the
conditional probabilities while enforcing the constraints,
expanding out to the general solution for ``error-correcting-codes.''

\subsection{One-versus-the-rest}

\label{one_vs_rest}

Common coding matrices include ``one-versus-the-rest'' in which
we take each class and train it against the rest of the
classes.
For $n_c=4$ it works out to:
\begin{eqnnon}
A = 
\begin{bmatrix}
1 & -1 & -1 & -1 \\
-1 & 1 & -1 & -1 \\
-1 & -1 & 1 & -1 \\
-1 & -1 & -1 & 1
\end{bmatrix}
\end{eqnnon}
or in the general case:
\begin{eqnnon}
	a_{ij}=2 \delta_{ij}-1
\end{eqnnon}

Probabilities for the one-versus-the-rest can be solved for directly by
simply writing out one side of the equation:
\begin{eqnnon}
	p_i = (r_i + 1)/2
\end{eqnnon}
The normalization constraint, (\ref{normalization}), can be enforced 
through the use of a Lagrange multiplier. See next section.

\subsection{One-versus-one}

\label{one_vs_one}

In a ``one-versus-one'' solution, we train each class against
every other class. For $n_c=4$:
\begin{eqnnon}
A = 
\begin{bmatrix}
-1 & 1 & 0 & 0 \\
-1 & 0 & 1 & 0 \\
-1 & 0 & 0 & 1 \\
0 & -1 & 1 & 0 \\
0 & -1 & 0 & 1 \\
0 & 0 & -1 & 1
\end{bmatrix}
\end{eqnnon}
The one-versus-one solution is used in LIBSVM \citep{Chang_Lin2011}.

Consider the following rearrangement of (\ref{non_hier}):
\begin{eqnarraynon}
	Q \vec p & = & \vec 0 \\
	q_{ij} & = & a_{ij} - r_i |a_{ij}|
\end{eqnarraynon}
We can include the normalization constraint, (\ref{normalization}), via
a Lagrange multiplier:
\begin{eqnnon}
	\min_{\vec p, \lambda} \left \lbrace | Q \vec p |^2 + \lambda(\vec 1 \cdot \vec p - 1) \right \rbrace
\end{eqnnon}
which produces the following linear system:
\begin{eqnarraynon}
	2 \sum_k q_{ki} q_{kj} p_j + \lambda & = & 0 \\
	\sum_j p_j & = & 1
\end{eqnarraynon}
It can be shown that with this solution for a 1-vs-1 coding matrix,
inequality constraints in (\ref{nonnegative}) are always satisfied
\citep{Wu_etal2004}.

\citet{Hsu_Lin2002} find that the one-vs.-one method is more accurate
for support vector machines (SVM) than either
error-correcting codes or one-vs.-the-rest.

\subsection{Error correcting codes}

Another common coding matrix is simply an arbitrary one: this is commonly
known as an ``error-correcting'' code \citep{Dietterich_Bakiri1995}.
This is often random, but may also be carefully designed 
\citep{Crammer_Singer2002}.
In principle this covers all the earliers cases, however in practice the
term can also refer more specifically to a random coding matrix.
We cover the solution of the general case, which includes a random
matrix, in Section \ref{general_case_solution}, below.

\subsection{Orthogonal codes}

\label{orthogonal}

To maximize the accuracy of an error-correcting coding matrix, 
\citet{Allwein_etal2000} and \citet{Windeatt_Ghaderi2002} show that
the distance between each of the columns, $| \vec a^{(i)} - \vec a^{(j)} |$, 
should be as large as possible,
where $\vec a^({i})$ is the $i$th column of the matrix $A$ and $i \ne j$. 
If we take the upright brackets once again to be a
Euclidian metric and assume that $A$ is ``strict'' then this 
reduces to minimizing the absolute value of the dot product,
$|\vec a^{(i)} \cdot \vec a^{(j)}|$.
The absolute value is used because a pair of columns that are the same except 
for a factor of -1 are degenerate.

In other words, 
%if the number of rows is equal to the number of classes, 
the optimal coding matrix will be orthogonal, $A^T A = n_p I$, where $I$ is
the $[n_c\times n_c]$ identity matrix. 
Orthogonal coding matrices 
are not hard to construct for certain class sizes, for instance:
\begin{eqnnon}
A = 
\begin{bmatrix}
1 & -1 & -1 & -1 \\
-1 & 1 & 1 & 1 \\
-1 & -1 & -1 & 1 \\
-1 & -1 & 1 & -1
\end{bmatrix}
\end{eqnnon}
\citet{Mills2017} provides a fast, simple and elegant iterative solution for solving
for conditional probabilities when using a ``strict'' orthogonal coding matrix.

\section{Solving for all the probabilities in the general case}

\label{general_case_solution}

\subsection{General comments}

\begin{figure}
	\includegraphics[width=0.9\textwidth]{config1.eps}
	\caption{Solving for the multi-class conditional probabilities with three clases.}
	\label{config1}
\end{figure}

Once the normalization constraint in (\ref{normalization}) has been applied,
the remaining inequality constraints for the minimization problem in
(\ref{minimization_problem}) to (\ref{nonnegative}) form a triangular
hyper-pyramid in a space of dimension $n_c-1$.
See Figure \ref{config1}.
Stating this as a more general constrained optimization problem, we have:
\begin{equation}
	\min_{\vec x} | E \vec x - \vec f |^2
	\label{minimization2}
\end{equation}
subject to:
\begin{equation}
	G \vec x \ge \vec h
	\label{inequality2}
\end{equation}
where $\vec x$ is a vector of length $n_c-1$ 
and $G$ is a $[n_c \times n_c-1]$ matrix.

Suppose we transform this into a new problem by interpolating between the
vertices of the hyper-pyramid.
First, we find the vertices by solving:
\begin{eqnnon}
	G_k \vec x_k = \lbrace h_i | i \ne k \rbrace
\end{eqnnon}
where $\vec x_k$ is the $k$th vertex of the hyper-pyramid and
we define $G_k$ as the matrix formed by ommitting the $k$th row of $G$.

We can locate any point inside the hyper-pyrmaid as follows:
\begin{eqnnon}
	x^*_i = \sum_i \gamma_i \vec x_i
\end{eqnnon}
where $k$ is an arbitrary index 
and $\vec \gamma = \lbrace \gamma_i \rbrace$ has the same properties as a probability:
\begin{eqnarraynon}
	\sum_i \gamma_i & = & 1 \\
	\vec \gamma & \ge & \vec 0
\end{eqnarraynon}
In other words, any minimization problem of the form of (\ref{minimization2})
and (\ref{nonnegative}) can be transformed into a problem of the same form
as (\ref{minimization_problem}) to (\ref{nonnegative}) and vice versa.

The first line of attack in solving constrained minimization problems of the
type we are discussing here are the Karesh-Kuhn-Tucker (KKT) conditions
which generalize Lagrange multipliers to inequality constraints
\citep{Lawson_Hanson1995,Boyd_Vandenberghe2004}.
For the minimization problem in (\ref{minimization_problem})-(\ref{nonnegative}), the KKT conditions translate to:
\begin{eqnnon}
	Q^T Q \vec p - Q^T \vec r + \lambda = \vec \mu
\end{eqnnon}
where:
\begin{eqnnon}
	\vec \mu \ge \vec 0
\end{eqnnon}
and:
\begin{eqnarraynon}
	\mu_i = 0 & \iff & p_i > 0 \\
	\mu_i > 0 & \iff & p_i = 0
\end{eqnarraynon}
or more succinctly:
\begin{eqnnon}
	\mu_i p_i = 0 ~ | ~ i = 1..n_c
\end{eqnnon}

Another important property of the problem is that it is completely {\it convex}.
A {\it convex function}, $c$, has the following property:
\begin{eqnnon}
	c \left \lbrace \gamma \vec x_0 + (1 - \gamma) \vec x_2 \right \rbrace
	\le \gamma c(\vec x_1) + (1 - \gamma) c(\vec x_2)
\end{eqnnon}
where $0 \le \gamma \le 1$ is a  coefficient.
Meanwhile, in a {\it convex set}, $C$:
\begin{eqnnon}
	\vec x_1 \in C \land \vec x_2 \in C \rightarrow 
\left \lbrace \gamma \vec x_1 + (1 - \gamma) \vec x_2 \right \rbrace \in C
\end{eqnnon}
\citep{Boyd_Vandenberghe2004}.
In a convex optimization problem, there can only be one extremum, moreover
simple, gradient descent algorithms should always eventually reach it.
Both the convexity property and the KKT conditions are used in the 
\citet{Lawson_Hanson1995} solution to inequality constrained least
squares problems.
See Section \ref{Lawson_Hanson}, below.



\subsection{Zadrozny solution}

\citet{Zadrozny2001} describes the following, iterative method of solving
for the probabilities using an arbitrary coding matrix:

\begin{itemize}
	\item For each $i=1, 2, ..., k$:
			Set $\tilde p_i := \frac{\sum_j n_j a_{ji} \tilde r_j}{\sum_j n_j a_{ji}}$
	\item Set $T := \sum_i \tilde p_i$; Set $\tilde p_i := \tilde p_i/T$
	\item Set $\tilde r_j := \sum_i a_{ji} \tilde p_i/\sum_i |a_{ji}|$
	\item Repeat until convergence.
\end{itemize}
where $n_j$ is the number of training samples in the $j$th class.
The technique minimizes the weighted Kullback-Leibler distance between actual and calculated
binary probabilities:
\begin{eqnnon}
	l(\vec x) = \sum_{i} n_i r_i \left [ \log \frac{r_i-1}{\tilde r_i-1} - \log \frac{1-r_i}{1-\tilde r_i} \right ]
\end{eqnnon}
as opposed to the usual Euclidean distance.
The method supplies probability estimates roughly as accurate as the others described here,
however our tests indicate that convergence is too slow to be useful.

\subsection{Lawson and Hanson solution}

\label{Lawson_Hanson}

\citet{Lawson_Hanson1995} describe an iterative solution to the following
inequality constrained least-squares problem:
\begin{eqnnon}
	\min_{\vec x} | E \vec x - \vec f |
\end{eqnnon}
subject to:
\begin{eqnnon}
	\vec x > \vec 0
\end{eqnnon}
where $E$ is an $[m \times n]$ matrix
and $\vec 0$ is a vector of all zeroes.

The solution is divided into a set $P$ containing the indices of 
all the non-zero values and a set $Z$ containing the indices of the 
zero values.
The algorithm is as follows:
\begin{enumerate}
	\item Set $P := \emptyset$; $Z:=1..n$
	\item Compute the $n$-vector $\vec \mu := E^T(\vec f - E \vec x)$.
	\item If the set $Z$ is empty or if $\mu_j \le 0$ for all $j \in Z$ go to Step 12.
	\item Find an index $t \in Z$ such that $\mu_i = \max \lbrace \mu_i: j \in Z \rbrace$.
	\item Move the index $t$ from set $Z$ to set $P$
	\item Solve the least squares problem $\min_{\vec z} | E_P \vec z - \vec f |$ where $E_P$=$\lbrace \vec e^{(j)} | j \in P \rbrace$ (columns of $E$ whose corresponding indices are in $P$).
	\item If $z_j > 0$ for all $j \in P$, set $\vec x:= \vec z$ and go to Step 2.
	\item Find an index $q \in P$ such that $q = \arg \min_j \lbrace x_j/(x_j - z_j): ~z_j \le 0;~ j \in P \rbrace$.
	\item Set $\alpha := x_q/(x_q - z_q)$
	\item Set $\vec x := \vec x + \alpha(\vec z - \vec x)$
	\item Move from set $P$ to set $Z$ all indices $j \in P$ for which $x_j = 0$. Go to step 6.
	\item The computation is completed.
\end{enumerate}

There are two loops to the algorithm. In the first loop, a new index is added
to the non-zero set in each iteration.
So long as the solution doesn't go out-of-bounds, this continues until all
the indices are added to the set $P$.
In the second loop, if it's found that one of the variables has gone out-of-bounds,
then the solution is adjusted to the nearest point in-bounds between the old solution
and the new.

\citet{Lawson_Hanson1995} describe several methods of combining
equality constraints such as the normalization constraint
in (\ref{normalization}) with the above constrained
least squares problem.
The most obvious is to repeat the variable substitution
described in Equations (\ref{reduce_dimension1}) to (\ref{reduce_dimension2}) until the excluded
probability is less-than-or-equal to $p_k\le 1$.


\section{Decision-trees}

\label{hierarchical}

The most obvious method of dividing up a multi-class problem into binary
classifiers is hierarchically using a {\it decision-tree} 
\citep{Cheong_etal2004, Lee_Oh2003}.
In this method, the classes are first divided into two partitions, then
those partitions are each divided into two partitions and so on until only
one class remains. The classification scheme is hierarchical, with all the
losing classes being excluded from consideration at each step.
Only the conditional probability of the winning class is calculated as the
product of all the returned conditional probabilities of the binary
classifiers.

Decision trees have the advantage that they are fast since on average they
require only $\log_2 n_c$ classifications and there is no need to solve a 
constrained matrix inverse. On the other hand, because there is less
information being taken into consideration, they may be less
accurate.

Interestingly, the same partitions created for a decision-tree can also
be used in a non-hierarchical scheme
to solve for all of the conditional probabilities. Consider the following
coding matrix for instance:
\begin{eqnnon}
A = 
\begin{bmatrix}
-1 & -1 & -1 & -1 & 1 & 1 & 1 & 1 \\
-1 & -1 & 1 & 1 & 0 & 0 & 0 & 0 \\
-1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & -1 & 1 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & -1 & -1 & 1 & 1 \\
0 & 0 & 0 & 0 & -1 & 1 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & -1 & 1
\end{bmatrix}
\end{eqnnon}
While there are only seven rows for eight classes, 
once we add in the constraint in (\ref{normalization}) the system becomes 
fully determined.

\subsection{Variations}

There are many variations on the method. \citet{Ramanan_etal2007} for instance train a 
one-versus-the-rest model at each level of the tree so that if the ``one''
class is returned, the lower levels of the tree are short circuited
and this class is selected for the final result. Otherwise, the one class
is left out of subsequent analysis. This is less a new method than simply
a means of shaping the tree appropriate to datasets with unbalanced
numbers of classes, for instance the ``Shuttle'' dataset \citep{King_etal1995}.

In a decision directed acyclic graph (DDAG), 
rather than testing one group against another, 
each node of the tree tests one class against another \citep{Platt_etal2000}. 
The losing class is excluded from subsequent analysis. 
The previous paragraph describes the ``tree'' version of the one-versus-the-rest. 
This is the tree version of one-versus-one. 
In a DDAG, there are multiple paths to the same node.

\subsection{Empirically designed trees}

\label{empirical}

\begin{figure}
	\includegraphics[width=0.9\textwidth]{landclasstree.eps}
	\label{landclasstree}
\end{figure}

Consider the following land-classification problem: you have remote-sensing measurements of four surface types: corn field, wheat field, evergreen forest and deciduous forest.
How do you divide up the tree to best classify the measurements into one of these four surface types?
A priori, it would make the most sense to first divide them by their more general grouping: field versus forest and then, once you have field, classify by type of field, or if you have forest, classify by the type of forest.
This is illustrated in Figure \ref{landclasstree}.

In this case we have prior knowledge of how the classes are related to one another.
On the other hand the classes may be too abstract to have any knowledge without examining the actual training data.
Many different methods of empirically designing both decision-trees and coding matrices have been shown in the literature.
\citet{Cheong_etal2004}, for instance, use a self-organizing-map (SOM)
\citep{Kohonen2000} to visualize the relationship between the classes while
\citet{Lee_Oh2003} use a genetic algorithm to optimize the decision-tree.

\citet{Benabdeslem_Bennani2006} design the tree by measuring the distance between
the classes and building a dendrogram.
\citet{Zhou_etal2008} use a similar method to design a coding matrix.
This seems the most straightforward approach and is interesting in that it reduces a very large problem involving probabilities into a much smaller one.
Consider the problem above: it stands to reason that the field and forest classes would be much more strongly separated than either of the sub-classes within.
That is the {\it interclass distance} between field and forest is larger.

How does one measure the interclass distance? Fundamentally this is a distance measure between two sets and there are many methods of determining it.
We could notate this as follows:
\begin{eqnarraynon}
	D_{ij} & = & \mathfrak D \left \lbrace P(\vec x|i),~P(\vec x|j) \right \rbrace \\
	       & \approx & D\left (\lbrace \vec x_k|~y_k=i \rbrace,~\lbrace \vec x_k|~y_k=j\rbrace \right )
\end{eqnarraynon}
where $\mathfrak D$ is a distance operator between two distributions and $D$ is a distance operator between two sets.

Consider the square of the distance between the means of the two classes divided by their standard deviations. Let:
\begin{eqnnon}
	\vec \mu_i = \frac{1}{n_i} \sum_{k|y_k=i} \vec x_k
\end{eqnnon}
be the mean of the $i$th class distribution where $n_i$ is the number of instances of that class, while:
\begin{eqnnon}
	\sigma_i = \frac{1}{n_i-1}\sqrt{\sum_{k|y_k=i}|\vec x_k - \vec \mu_i|^2}
\end{eqnnon}
is the standard deviation.
Then let the distance between the two classes be:
\begin{eqnnon}
	D_{ij}=\frac{|\vec \mu_j - \vec \mu_i |^2}{\sqrt{\sigma_i \sigma_j}}
\end{eqnnon}
That is, the closer the centers of the two classes, the shorter the distance, while the wider each class is, the farther the distance.
	
This would work well if each of the classes is quite distinct and clustered around a strong center.
But for more diffuse classes, especially those with multiple centers, it would make more sense to use a metric designed specifically for sets rather than this somewhat crude adaptation of a vector metric.
In this regard, the Hausdorff metric seems tailor-made for this application.

For training samples from a pair of classes---two finite sets---the Hausdorff distance works out to \citep{Ott1993, Gulick1992}:
\begin{eqnnon}
D_{Hij} = \max \left \lbrace \min_k | \vec x_k - \vec x_l|~,~\min_l | \vec x_k - \vec x_l| ~ ;~y_k=i;~y_l=j \right \rbrace
\end{eqnnon}

\section{Unifying framework}

Since there are many ways of solving the multi-class classification problem,
we present here a descriptive control language that unifies many of the ideas
presented in the previous sections.
This is not a "one-size-fits-all" solution, but rather a means of specifying
a particular partitioning that best suits the problem at hand.
This partitioning could be arrived at either through prior knowledge, 
or empirically by measuring the distance between the classes, for instance--or
by simply exhaustively testing different configurations.

In Backus-Naur form (BNF) the control language looks like this:

\begin{tabular}{lcl}
$<$branch$>$ & ::= & $<$model$>$ ``\{'' $<$branch-list$>$ ``\}'' $|$ $<$CLASS$>$\\
$<$model$>$  & ::= & $<$TWOCLASS$>$ $|$ $<$partition-list$>$\\
$<$branch-list$>$ & ::= & $<$branch$>$ $|$ $<$branch-list$>$ $<$branch$>$\\
$<$partition-list$>$ & ::= & $<$partition$>$ $|$ $<$partition-list$>$ $<$partition$>$\\
$<$partition$>$ & ::= & $<$TWOCLASS$>$ $<$class-list$>$ `` / '' $<$class-list$>$ ``;''\\
$<$class-list$>$ & ::= & $<$CLASS$>$ $|$ $<$class-list$>$ `` '' $<$CLASS$>$
\end{tabular}.

where $<$CLASS$>$ is a class value between 0 and $n_c-1$.  It is used in two senses.
It may be one of the class values in a partition in a non-hierarchical model.
In this case it's value is relative, that is local to the non-hierarchical model.
It may also be the class value returned
from a top level partition in the hierarchy in which case it's value is absolute.
$<$TWOCLASS$>$ is a binary classification model.
This could either be the name of model that has already been trained or it
could be a list of options or specifications used to train said model.

For example, a one-versus-one specification for four classes would look like
this:

\begin{verbatim}
  model01 0 / 1;
  model02 0 / 2;
  model03 0 / 3;
  model12 1 / 2;
  model13 1 / 3;
  model23 2 / 3;
  {0 1 2 3}
\end{verbatim}

while a one-versus-the-rest specifications, also for four class, would look
like this:

\begin{verbatim}
  model0 1 2 3 / 0;
  model1 0 2 3 / 1;
  model2 0 1 3 / 2;
  model3 0 1 2 / 3;
  {0 1 2 3}
\end{verbatim}

A hierarchical specification might look like this:

\begin{verbatim}
  TreeVsField {
    EvergreenVsDeciduous {0 1}
    CornVsWheat {2 3}
  }
\end{verbatim}

The framework allows the two methods,
 hiearchical and non-hierarchical, 
to be combined
as in the following, nine-class example:

\begin{verbatim}
  TREESvsFIELD 0 / 1;
  TREESvsWATER 0 / 2;
  FIELDvsWATER3 1 / 2;
  {
    DECIDUOUSvsEVERGREEN 0 / 1;
    DECIDUOUSvsSHRUB 0 / 2;
    EVERGREENvsSHRUB 1 / 2;
    {1 2 3}
    CORNvsWHEAT 0 / 1;
    CORNvsLEGUME 0 / 2;
    WHEATvsLEGUME 1 / 2;
    {4 5 6}
    FRESHvsSALT 0 / 1;
    FRESHvsMARSH 0 / 2;
    SALTvsMARSH 1 / 2;
    {7 8 9}
  }
\end{verbatim}

The above demonstrates how the feature might be useful
on a hypothetical surface-classification problem with the key as follows:

\begin{tabular}{ll}
	0 & Deciduous forest \\
	1 & Evergreen forest \\
	2 & Shrubs \\
	3 & Corn field \\
	4 & Wheat field \\
	5 & Legume field \\
	6 & Freshwater \\
	7 & Saltwater \\
	8 & Marsh
\end{tabular}

\section{Numerical trials}

We wish to test a synthesis of the ideas contained in this review on some real
datasets.
To this end, we will test eight different datasets using six configurations 
solved using four different methods.
The configurations are: one-vs-one, one-vs.-rest, orthogonal partioning,
``adjacent'' partitioning (see below), an arbitray tree, and a tree generated
from a bottom-up dendrogram using the Hausdorf metric.
The solution methods are: constrained least squares as described
in Section \ref{Lawson_Hanson}, matrix inverse which is specific to one-vs.-one,
the iterative method designed orthogonal partitioning 
(see Section \ref{orthogonal}), and recursively which is appropriate
only for hierarchical or tree-based configurations.

The control language allows us to represent any type of multi-class 
configuration relatively succinctly, including different parameters
used for the binary classifiers.
To illustrate the operation of the empirical partitioning, here are
control files for the shuttle dataset.
The arbitrary, balanced tree is as follows:
\begin{verbatim}
shuttle_hier {
  shuttle_hier.00 {
    0
    shuttle_hier.00.01 {
      1
      2
    }
  }
  shuttle_hier.01 {
    shuttle_hier.01.00 {
      3
      4
    }
    shuttle_hier.01.01 {
      5
      6
    }
  }
}

\end{verbatim}

While the empirically-designed tree is:
\begin{verbatim}
shuttle_emp {
  shuttle_emp.00  {
    shuttle_emp.00.00 {
      shuttle_emp.00.00.00 {
        shuttle_emp.00.00.00.00 {
          shuttle_emp.00.00.00.00.00 {
            2
            1
          }
          5
        }
        6
      }
      3
    }
    4
  }
  0
}
\end{verbatim}

\begin{table}
	\caption{Class distribution in the shuttle dataset.}
	\label{shuttle_dist}
	\begin{center}
	\begin{tabular}{|ll|}
		\hline
		class & number \\
		\hline
		0 & 45586 \\
		1 & 50 \\
		2 & 171 \\
		3 & 8903 \\
		4 & 3267 \\
		5 & 10 \\
		6 & 13 \\
		\hline
	\end{tabular}
	\end{center}
\end{table}

\begin{figure}
	\includegraphics[width=\textwidth]{shuttle_tree.eps}
	\caption{Multiclass decision-trees for the shuttle dataset. 
	In (a) we rank them mechanically whereas in (b) the tree is a dendrogram based on the Hausdorff distance between each class.}\label{shuttle_tree}
\end{figure}

Note that the shuttle dataset is very unbalanced, 
as listed in Table \ref{shuttle_dist},
hence  the empirically-designed tree looks more like a chain
as illustrated in Figure \ref{shuttle_tree}.
To solve the hierarchical models using least-squares, hierarchical models were first translated to non-hierarchical, as discussed in Section \ref{hierarchical}.
The above, for instance, becomes:
\begin{verbatim}
shuttle_emp 0 1 2 3 4 5 / 6;
shuttle_emp.00 0 1 2 3 4 / 5;
shuttle_emp.00.00 0 1 2 3 / 4;
shuttle_emp.00.00.00 0 1 2 / 3;
shuttle_emp.00.00.00.00 0 1 / 2;
shuttle_emp.00.00.00.00.00 0 / 1;
{ 2 1 6 5 3 4 0}
\end{verbatim}

The ``adjacent'' partitioning is as follows:
\begin{verbatim}
shuttle_adj-00 0 / 1 2 3 4 5 6;
shuttle_adj-00 0 1 / 2 3 4 5 6;
shuttle_adj-00 0 1 2 / 3 4 5 6;
shuttle_adj-00 0 1 2 3 / 4 5 6;
shuttle_adj-00 0 1 2 3 4 / 5 6;
shuttle_adj-00 0 1 2 3 4 5 / 6;
{0 1 2 3 4 5 6}
\end{verbatim}
with corresponding coding matrix:
\begin{eqnnon}
A = 
\begin{bmatrix}
-1 & 1 & 1 & 1 & 1 & 1 & 1 \\
-1 & -1 & 1 & 1 & 1 & 1 & 1 \\
-1 & -1 & -1 & 1 & 1 & 1 & 1 \\
-1 & -1 & -1 & -1 & 1 & 1 & 1 \\
-1 & -1 & -1 & -1 & -1 & 1 & 1 \\
-1 & -1 & -1 & -1 & -1 & -1 & 1 \\
\end{bmatrix}
	\label{adjacent}
\end{eqnnon}
The rational for using it will be explained in Section \ref{results}, below.

\subsection{Data and software}

\begin{table}
	\caption{Summary of datasets used in the analysis}\label{datasets}
	\begin{tabular}{|l|lllll|}
	\hline
	Name & $D$ & Type & $n_c$ & $N$ & Reference \\\hline \hline
	letter &  16 & integer & 26 & 20000 & {\small \citep{Frey_Slate1991}}\\
	pendigits & 16 & integer & 10 & 10992 & {\small \citep{Alimoglu1996}}\\
	usps & 256 & float & 10 & 9292 & {\small \citep{Hull1994}}\\
	segment & 19 & float & 7 & 2310 & {\small \citep{King_etal1995}} \\
	sat & 36 & float & 6 & 6435 & {\small \citep{King_etal1995}}\\
	urban & 147 & float & 9 & 675 & {\small \citep{Johnson2013}} \\
	shuttle & 9 & float & 7 & 58000 & {\small \citep{King_etal1995}}\\
	humidity & 7 & float & 8 & 8600$^*$ & {\small \citep{Mills2009}} \\
	\hline
\end{tabular}
	\vspace{1 ex}

	\raggedright $^*$ Humidity dataset has been sub-sampled to keep training times reasonable.
\end{table}

The datasets tested are as follows: 
``pendigits'' and ``usps'' are both digit recognition problems 
\citep{Alimoglu1996, Hull1994};
the ``letter'' dataset is another text-recognition problem 
that classifies letters rather than numbers 
\citep{Frey_Slate1991};
the ``segment'' dataset is a pattern-based image-classification problem;
the ``sat'' dataset is a satellite land-classification problem;
the ``shuttle'' dataset predicts different flight configurations on the
space shuttle \citep{Michie_etal1994, King_etal1995};
the ``urban'' dataset is another pattern-recognition dataset for urban land cover
\citep{Johnson2013};
and the ``humidity'' dataset classifies humidity values based on satellite
radiometry \citep{Mills2009}.
The characteristics of each dataset are summarized in Table \ref{datasets}.

The base binary classifier used to test the ideas in this paper is a 
support vector machine (SVM) \citep{Mueller_etal2001}.
We use the LIBSVM \citep{Chang_Lin2011} to perform the training
using the \verb/svm-train/ command.
LIBSVM is a simple yet powerful library for SVM that implements multiple
kernel types and includes two different regularization methods.
It was developed by Chih-Chung Chang and Chih-Hen Lin of the National
Taiwan University in Taipei.
LIBSVM can be downloaded at: \url{https://www.csie.ntu.edu.tw/~cjlin/libsvm}.

Everything else was done using the libAGF library \citep{Mills2011,Mills2018}
which includes extensive codes for generalized multi-class classification.
These codes interface seamlessly with LIBSVM and provide for automatic
generation of multiple types of control file using the \verb/print_control/
command.
Control files are used to train the binary classifiers and then
to make predictions using the \verb/multi_borders/ and \verb/classify_m/
commands, respectively.
Before making predictions, the binary classifiers were unified to eliminate
duplicate support vectors using the \verb/mbh2mbm/ command, thus improving
efficiency.
LibAGF may be downloaded at: \url{https://github.com/peteysoft/libmsci}.

To evaluate the conditional probabilities we use the Brier score
\citep{Brier1950, Jolliffe_Stephenson2003}:
\begin{eqnnon}
B=\sqrt{\frac{1}{n} \sum_{i=1}^{n} \sum_{j=1}^{n_c} \left ( \tilde p_{ij} - \delta_{jy_i} \right )^2}
\end{eqnnon}
where $n$ is the number of test samples.

\begin{table}
	\caption{Key for Tables \ref{training1} through \ref{Brier4}.}\label{key}
	\begin{tabular}{|lll|}
		\hline
		term & meaning & cross-ref. \\
		\hline\hline
		config. & configuration of multiclass partitioning & Equation (\ref{mapping}) \\
		method & solution method for computing probabilities & Equation (\ref{maximum_likelihood}) \\
		\hline
		1 vs. 1 & one-versus-one partitioning & Section \ref{one_vs_one} \\
		1 vs. rest & one-versus-the-rest partitioning & Section \ref{one_vs_rest} \\
		ortho. & orthogonal coding & Section \ref{orthogonal} \\
		adj. & adjacent partitioning & Equation (\ref{adjacent}) \\
		hier. & ``hierarchical'' or decision-tree partitioning & Section \ref{hierarchical} \\
		emp. & empirically-designed decision-tree & Section \ref{empirical} \\
		\hline
		lsq. & Lawson and Hanson constrained least-squares & Section \ref{Lawson_Hanson} \\
		inv. & matrix inverse solution & Section \ref{one_vs_one} \\
		iter. & iterative solution for orthogonal codes & Section \ref{orthogonal} \\
		rec. & recursive ascent of decision-tree & Section \ref{hierarchical} \\
		\hline
	\end{tabular}
\end{table}


\section{Results and discussion}

\label{results}

\input{results2018_09_06.tex}

\begin{figure}[htp]
	\begin{boxedminipage}{\textwidth}
		\begin{small}
		\begin{verbatim}
segment_emp {
  segment_emp.00 {
    segment_emp.00.00 {
      segment_emp.00.00.00 {
        segment_emp.00.00.00.00 {
          segment_emp.00.00.00.00.00 {
            PATH
            BRICKFACE
          }
          GRASS
        }
        WINDOW
      }
      SKY
    }
    FOLIAGE
  }
  CEMENT
}
		\end{verbatim}
		\end{small}
	\end{boxedminipage}
	\caption{Control file for a multi-class decision-tree designed empirically for the segment dataset. Image type is used for the class labels.}
	\label{segment}
\end{figure}

\begin{figure}[htp]
	\begin{boxedminipage}{\textwidth}
		\begin{small}
		\begin{verbatim}
pendigits_emp {
  pendigits_emp.00 {
    pendigits_emp.00.00 {
      pendigits_emp.00.00.00 {
        pendigits_emp.00.00.00.00 {
          pendigits_emp.00.00.00.00.00 {
            pendigits_emp.00.00.00.00.00.00 {
              pendigits_emp.00.00.00.00.00.00.00 {
                7
                2
              }
              3
            }
            1
          }
          pendigits_emp.00.00.00.00.01 {
            8
            5
          }
        }
        9
      }
      4
    }
    6
  }
  0
}
		\end{verbatim}
		\end{small}
	\end{boxedminipage}
	\caption{Control file for a multi-class decision-tree designed empirically for the pendigits dataset.}
	\label{pendigits}
\end{figure}

\begin{figure}[htp]
	\begin{boxedminipage}{\textwidth}
		\begin{small}
		\begin{verbatim}
sat_emp {
  sat_emp.00 {
    sat_emp.00.00 {
      sat_emp.00.00.00 {
        sat_emp.00.00.00.00 {
          VERY DAMP GREY SOIL
          DAMP GREY SOIL
        }
        RED SOIL
      }
      GREY SOIL
    }
    STUBBLE
  }
  COTTON CROP
}
		\end{verbatim}
		\end{small}
	\end{boxedminipage}
	\caption{Control file for a multi-class decision-tree designed empirically for the sat dataset. Surface-type is used for the class labels.}
	\label{sat}
\end{figure}


Results are shown Tables \ref{training1} through \ref{Brier4} with the
key given in Table \ref{key}.
If we take the results for these eight datasets as being representative,
there are several conclusions that can be made.
The first is that despite the seeming complexity of the problem, a 
``one-size-fits-all'' approach seems perfectly adequate for most datasets.
Moreover, this approach is the one-versus-one method,
which we should note is used exclusively in LIBSVM \citep{Chang_Lin2011}.
One-vs.-one has other advantages such as the simplicity of solution:
a standard linear solver such as Gaussian elimination,
QR decomposition or SVD is sufficient,
as opposed to a complex, iterative scheme.

Further, the partitioning used does not even appear all that
critical in most cases.
Even a sub-optimal method, such as the ``adjacent'' partioning, which makes
little sense for datasets in which the classes have no ordering,
gives up relatively little accuracy to more sensible methods on most datasets.
For the urban dataset it is actually superior, suggesting that there is
some kind of ordering to the classes which are: trees, grass, soil, concrete,
asphalt, buildings, cars, pools, shadows.
If classification speed is critical, a hierarchical approach will trade 
off accuracy to save some compute cycles with 
$O(\log n_c)$ performance instead of $O(n_c^2)$.
Accuracy lost is again quite dependent on the dataset.

Not only can sub-optimal partitionings produce reasonable results, 
but approximate solution methods can also be used without much penalty.
For instance, the iterative method for orthogonal coding matrices
outlined in \citet{Mills2017}, when applied as a general method
actually gives up very little in accuracy,
even though it is not optimal in the least-squares sense.
(These results are not shown.)

A data-dependent decision-tree design can provide a small but significant
increase in accuracy over a more arbitrary tree.
An interesting side-effect is
that it can also improve both training and classificaton speed.
Strangely, the technique worked better for the character recognition datasets than
the image classification datasets.
The groupings found did not always correspond with what might be
expected from intuition.
In a pattern-recognition dataset, it might not always be clear  
how different image types should be related anyway, as in
the segment dataset, Figure \ref{segment}.
For another example, the control file for the pendigits dataset is shown in Figure \ref{pendigits}.
We can see how 8 and 5 might be related, but it is harder to
understand how 3 and 1 are related to 7 and 2.
On the other hand, the arrangement might turn out almost exactly
expected as in the sat dataset, Figure \ref{sat}.
This is also the only patter-recognition dataset for which the method worked.

Unfortunately the approach used here isn't able to match the one-versus-one 
arrangement in accuracy but
this does not preclude cleverer schemes producing larger gains.
Using similar techniques, \citet{Benabdeslem_Bennani2006} and \citet{Zhou_etal2008} are both able to beat one-vs.-one, but only by a narrow margin.

Finally, some datasets may have special characteristics that can be exploited
to produce more accurate results through the multi-class configuration.
While this was the original thesis behind this paper,
only one example was found in this group of eight datasets, 
namely the humidity dataset.
Because the classes are a discretized continuous variable, they have an
ordering. As such, it is detrimental to split up consecutive classes more
than absolutely necessary and the ``adjacent'' partitioning is the most
accurate. Meanwhile, the one-vs.-rest configuration performs abysimally while
the one-vs.-one performs well enough, but worse than all the other methods
save one.
The excellent performance of the adjacent configuration for the urban
dataset suggests that searching for an ordering to the classes might be a
useful strategy for improving accuracy.
This could be done using inter-set distance in a manner similar to the empirical hierarchical method.

Since the results present a somewhat mixed bag, it would be useful to
have a framework and toolset with which to explore different methods of
building up multi-class classification models so as to optimize classification
accuracy, accuracy of conditional probabilities and speed.
This is is what we have tried to lay out in this paper.


\bibliography{../agf_bib,multi2,../svm_accel/svm_accel,../pwl}

\end{document}

